\chapter{Kết hợp học sâu với học tăng cường}
\begin{quote}
\textit{Học sâu đạt được nhiều kết quả khả quan trong nhiều lĩnh vực như xử lý ngôn ngữ tự nhiên, nhận diện giọng nói, nhận dạng đối tượng ... Ưu điểm của học sâu là xấp xỉ hàm tốt, rút trích đặc trưng ở mức high-level. \\
Trong chương này sẽ trình bày chi tiết những phương pháp mà chúng em áp dụng để giải quyết bài toán "tự động chơi game". Chúng em sử dụng phương pháp Q-Learning, một phương pháp đánh giá được sử dụng phổ biến trong nhiều công trình nghiên cứu về học tăng cường gần đây, để đánh giá hàm giá trị của hành động cho bài toán này. Ngoài ra, chúng em cũng thực hiện xấp xỉ hàm giá trị của bài toán bằng mạng nơ-ron và áp dụng hai kỹ thuật Experience Replay và Fixed Q-targets để tăng tính ổn định cho quá trình học.}
\end{quote}
%
%\section{Đánh giá chính sách bằng phương pháp Q-Learning}
%	\begin{itemize}
%		\item Phân biệt học trên chính sách và ngoài chính sách (On-policy/Off-policy)
%		\item Thuật toán Q-Learning
%	\end{itemize}
%	
%\section{Xấp xỉ hàm giá trị bằng mạng neuron}
%	\begin{itemize}
%		\item Vấn đề trong bài toán thực tế: tập trạng thái lớn hoặc vô hạn
%		\item Phương pháp xấp xỉ hàm giá trị bằng mạng neuron
%		\item Giới thiệu mạng Convolutional Neural Network
%	\end{itemize}
%	
%\section{Kỹ thuật làm tăng tính ổn định cho quá trình học}
%	\begin{itemize}
%		\item Experience Replay
%		\item Fixed Q-targets
%	\end{itemize}
%	
%\section{Đề xuất phương pháp cải tiến (Nếu có)}
	