\chapter{Kiến thức nền tảng}
\ifpdf
\graphicspath{{Chapter2/Chapter2Figs/PNG/}{Chapter2/Chapter2Figs/PDF/}{Chapter2/Chapter2Figs/}}
\else
\graphicspath{{Chapter2/Chapter2Figs/EPS/}{Chapter2/Chapter2Figs/}}
\fi
\begin{quote}
	\textit{Chương này trình bày những kiến thức nền tảng của học tăng cường. 
	Trong phần đầu tiên, chúng em trình bày định nghĩa của các thành phần cơ bản trong học tăng cường. 
	Tiếp đó, mô hình ``Markov Decision Processes'' được giới thiệu; mô hình này dùng để mô hình hoá các bài toán học tăng cường khác nhau về chung một ``framework'' cố định.
	Từ đó, ta có thể giải các bài toán học tăng cường bằng cách giải bài toán trên mô hình này.
	Cuối cùng, chúng em trình bày thuật toán ``Q-learning'' - thuật toán học tăng cường được chúng em sử dụng trong bài toán tự động chơi game.}
\end{quote}

\section{Các thành phần cơ bản của học tăng cường}
\subsection{Hệ thống và môi trường}
Trong học tăng cường, thành phần cần ``học'' và đưa ra quyết định được gọi chung là \textit{hệ thống} (agent).
Thành phần này tương tác trực tiếp tới các đối tượng ``bên ngoài'' được gọi là \textit{môi trường} (environment).
Một cách đơn giản, môi trường ở đây chính là những thành phần ``ẩn'' mà hệ thống không kiểm soát trực tiếp được; để thay đổi môi trường, hệ thống phải tương tác với môi trường bằng cách thực hiện hành động.
Sự tương tác này diễn ra tại các mốc thời gian nhất định.
Tại mỗi thời điểm, hệ thống lựa chọn hành động dựa trên những thông tin nhận được từ môi trường.
Những thông tin này bao gồm:
\begin{itemize}
	\item \textbf{Trạng thái} (state): những thông tin về môi trường xung quanh mà hệ thống nhận được. 
	Ví dụ trong đánh cờ, trạng thái có thể là vị trí những quân cờ đang có trên bàn cờ. 
	Môi trường trong bài toán học tăng cường có thể mang yếu tố ngẫu nhiên (stochastic environment): một hành động có thể có dẫn đến những kết quả khác nhau.
	Ví dụ như trong trò chơi đổ xúc xắc, giá trị của xúc xắc là hoàn toàn ngẫu nhiên.
	Vì vậy, nếu môi trường thay đổi dựa vào kết quả của xúc xắc thì trạng thái cũng thay đổi một cách ngẫu nhiên.
	Ký hiệu của trạng thái là $s$.
	\item \textbf{Điểm thưởng} (reward): giá trị số thực mà môi trường trả về cho hệ thống; giá trị điểm thưởng này nói lên độ tốt của một hành động tại một trạng thái nào đó. 
	Với ví dụ đánh cờ, điểm thưởng mà hệ thống có thể nhận được từ môi trường ở cuối ván cờ là +1 nếu hệ thống thắng, -1 nếu hệ thống thua.
	Trong quá trình đánh cờ, điểm thưởng có thể là 0 cho mỗi nước cờ mà hệ thống thực hiện.
	Có thể thấy trong ví dụ này, điểm thưởng không nói rõ nước cờ nào là tốt mà chỉ đánh giá tổng quát cho cả ván đấu.
	Chính vì vậy, điểm thưởng có ý nghĩa khác biệt so với nhãn của phương pháp học có giám sát.
	Tương tự như trạng thái, điểm thưởng cũng có thể ngẫu nhiên.
	Với ví dụ xúc xắc, nếu điểm thưởng là giá trị của xúc xắc thì điểm thưởng là ngẫu nhiên.
	Nếu ta chơi hai lần và thực hiện cùng một hành động tại cùng một trạng thái thì điểm thưởng vẫn có thể khác nhau.
	Điểm thưởng được ký hiệu là $r$. 
\end{itemize}
Từ trạng thái và điểm thưởng nhận được, hệ thống chọn hành động phù hợp để đạt được điểm thưởng cao nhất.

Hệ thống và môi trường tương tác với nhau một cách tuần tự.
Thời điểm tương tác (được ký hiệu là $t$) được đánh số tăng dần: $t = 0, 1, 2,..., T$ (với $T$ là thời điểm kết thúc).
Tại thời điểm $t$, hệ thống nhận được trạng thái hiện tại $\mathit{S_t} \in \mathcal{S}$, với $\mathcal{S}$ là tập các trạng thái có thể có. 
Dựa vào trạng thái nhận được, hệ thống thực hiện một hành động $\mathit{A_t} \in \mathcal{A}$, trong đó $\mathcal{A}$ là tập các hành động có thể thực hiện. 
Hành động này làm cho môi trường thay đổi.
Tại thời điểm $t+1$ sau đó, hệ thống nhận được giá trị điểm thưởng $\mathit{R_{t+1}} \in \mathbb{R}$ cùng với trạng thái tiếp theo $\mathit{S_{t+1}}$.
Giá trị $R_{t+1}$ là điểm thưởng nhận được do hành động $\mathit{A_t}$ vừa làm thay đổi môi trường (chứ không phải tổng điểm thưởng từ lúc bắt đầu).
Quá trình tương tác giữa hệ thống và môi trường cứ tiếp tục lặp lại cho đến khi môi trường trả về trạng thái kết thúc.
Quá trình tương tác này được mô tả trong hình \ref{AgentEnvironment}.

\begin{figure}
	\centering
	\includegraphics[width=.8\textwidth]{AgentEnvironment}
	\caption{Quá trình tương tác giữa hệ thống và môi trường}
	\label{AgentEnvironment}
\end{figure}

Các thành phần của hệ thống gồm có:
\begin{itemize}
	\item \textbf{Chính sách} $\pi$ là cách hệ thống lựa chọn hành động dựa vào trạng thái hiện tại.
	Tại thời điểm $t$, khả năng một hành động $a$ được chọn ở trạng thái $s$ là $\pi(a \mid s) = \mathbb{P}[\mathit{A_t} = a \mid \mathit{S_t} = s]$. 
	Nếu chính sách luôn chọn một hành động với xác suất là $1$ thì chính sách đó gọi là \textit{đơn định} (deterministic).
	Chính sách đơn định lúc này có thể coi là một ánh xạ từ tập trạng thái sang tập hành động: $a = \pi(s)$.
	Để đạt nhiều điểm thưởng nhất, hệ thống cần có một chính sách chọn lựa hành động phù hợp. 
	Những phương pháp học tăng cường thường tập trung thay đổi dần dần các chính sách của hệ thống để đạt được mục tiêu trên.
	\item \textbf{Hàm giá trị} dùng để đánh giá trạng thái hoặc hành động tại trạng thái là ``tốt'' đến mức nào.
	Để đánh giá một trạng thái, ta có thể thử tương tác với môi trường và tính tổng điểm thưởng nhận được từ khi gặp trạng thái đó cho đến khi kết thúc.
	Tuy nhiên, do môi trường có thể mang tính ngẫu nhiên, tổng điểm thưởng này có thể khác nhau ở những lần tương tác khác nhau.
	Vì vậy, giá trị trạng thái được định nghĩa là \textbf{kỳ vọng} tổng điểm thưởng (lấy trung bình vô số lần tương tác).
	Lúc này, giá trị của một trạng thái $s$ theo chính sách $\pi$ được ký hiệu $v_{\pi}(s)$ là kỳ vọng tổng điểm thưởng mà hệ thống có thể nhận được bắt đầu từ trạng thái $s$ về sau.
	Lý do có chính sách $\pi$ trong định nghĩa này là do điểm thưởng nhận được sẽ phụ thuộc vào cách hệ thống chọn hành động.
	Tương tự, để đánh giá một hành động tại một trạng thái, ta định nghĩa hàm giá trị hành động.
	Giá trị của việc thực hiện hành động $a$ tại trạng thái $s$ rồi tương tác theo chính sách $\pi$ được ký hiệu $q_{\pi}(s,a)$.
	Giá trị này thể hiện là kỳ vọng tổng điểm thưởng mà hệ thống có thể nhận được kể từ sau khi thực hiện hành động $a$ tại trạng thái $s$ và thực hiện các hành động tiếp theo dựa vào chính sách $\pi$. 
	\item \textbf{Mô hình}. Trong một số bài toán học tăng cường, hệ thống có thể xây dựng mô hình cho riêng mình để mô phỏng lại môi trường. 
	Qua đó cho phép hệ thống có thể suy luận hoặc dự đoán những thông tin của môi trường trong tương lai.
\end{itemize}	

\subsection{Tổng điểm thưởng}
Tổng điểm thưởng (return) $\mathit{G_t}$ xác định lượng điểm thưởng mà hệ thống nhận được kể từ thời điểm $t$ trở đi. 
Ta có thể định nghĩa giá trị này bằng:
\begin{equation}
	\mathit{G_t} = \mathit{R_{t+1}} + \mathit{R_{t+2}} + ... + \mathit{R_{T}}
\end{equation}	
ở đây $T$ là thời điểm cuối cuối cùng hệ thống tương tác với môi trường.
Tuy nhiên, tổng này có thể không hữu hạn nếu như độ dài một lần tương tác không được giới hạn.

Từ đây, ta có thể sử dụng một định nghĩa tổng điểm thưởng luôn hữu hạn:
\begin{equation}
\mathit{G_t} = \mathit{R_{t+1}} + \gamma\mathit{R_{t+2}} + \gamma^{2}\mathit{R_{t+3}} + ... + \gamma^{T-t-1}\mathit{R_{T}} = \sum_{k=0}^{\infty}\gamma^{k}\mathit{R_{t+k+1}}
\end{equation}
Trong đó $\gamma$ là hệ số ``giảm điểm thưởng'' (discount factor) với giá trị $0\leqslant \gamma \leqslant 1$.
Đây là định nghĩa tổng điểm thưởng được dùng trong các bài toán học tăng cường.
Lý do chính để các nhà nghiên cứu chọn định nghĩa này là để đơn giản hoá về mặt toán học.
Tuy vậy, định nghĩa này cũng có một số lợi ích khác.
Nếu như cần dự đoán giá trị tổng điểm thưởng tại thời điểm $t$ (một việc mà ta sẽ phải làm thường xuyên), các giá trị điểm thưởng ở càng xa thì khả năng ta dự đoán đúng càng thấp.
Vì vậy, trọng số của các điểm thưởng ở càng xa thời điểm hiện tại cần phải giảm dần.
Với $\gamma = 0$, hệ thống chỉ quan tâm điểm thưởng ngay lập tức mà không quan tâm về lâu dài.
Ngược lại, $\gamma$ càng gần $1$ thì hệ thống càng quan tâm một cách dài hạn hơn về điểm thưởng.

\section{Mô hình Markov Decision Processes}
%	\begin{itemize}
%			\item Các thành phần MDP
%			\item Ví dụ cho mô hình MDP
%			\item Phương trình Bellman
%			\item Qui trình đánh giá chính sách: Kỹ thuật qui hoạch động
%			\item Qui trình cải thiện chính động: Kỹ thuật qui hoạch động
%	\end{itemize}
\subsection{Định nghĩa mô hình Markov Decision Processes}
Mô hình Markov Decision Processes (MDP) được sử dụng để mô hình hóa bài toán học tăng cường một cách hình thức. 
Cụ thể, MDP là một bộ bao gồm 5 thành phần $<\mathcal{S, A, P, R, \gamma}>$:
\begin{itemize}
	\item $\mathcal{S}$: tập trạng thái hữu hạn có thể có của môi trường.
	\item $\mathcal{A}$: tập hữu hạn những hành động mà hệ thống có thể thực hiện để tương tác với môi trường.
	Tổng quát hơn, tại mỗi trạng thái, hệ thống có thể thực hiện những hành động khác nhau.
	Khi đó, tập hành động tại trạng thái $s$ được ký hiệu là $\mathcal{A}(s)$.
	\item $\gamma$: hệ số giảm điểm thưởng có giá trị thỏa $0\leqslant \gamma \leqslant 1$ thể hiện mức độ tin tưởng về giá trị điểm thưởng nhận được ở tương lai.
	\item $\mathcal{P}$: ma trận xác suất chuyển trạng thái. 
	Trong đó $\mathcal{P}_{ss'}^{a}$ là xác suất chuyển đến trạng thái $s'$ khi hệ thống đang ở trạng thái $s$ và thực hiện hành động $a$.
	\begin{equation}
	\mathcal{P}_{ss'}^{a} = \mathbb{P}[\mathit{S_{t+1}} = s' \mid \mathit{S_{t}} = s, \mathit{A_{t}} = a]
	\end{equation}
	\item $\mathcal{R}$: ma trận điểm thưởng của từng \textit{bộ (trạng thái, hành động)}. 
	$\mathcal{R}_{s}^a$ là giá trị kỳ vọng điểm thưởng nhận được \textbf{ngay lập tức} khi hệ thống thực hiện hành động $a$ ở trạng thái $s$.
	\begin{equation}
	\mathcal{R}_{s}^a = \mathbb{E}[\mathit{R_{t}} \mid \mathit{S_{t}} = s, \mathit{A_{t}} = a]
	\end{equation}				
\end{itemize}

\textbf{Ví dụ mô hình MDP của bài toán robot thu gom soda (ví dụ chỉnh sửa từ \cite{sutton1998introduction}):} công việc của robot này là thu lượm những lon soda đã được uống hết trong văn phòng. 
Robot có những cảm biến để xác định những lon soda này, bánh xe và cánh tay để di chuyển và gắp nhặt những lon này bỏ vào thùng. 
Robot hoạt động bằng pin sạc. 
Hệ thống điều khiển của robot có chức năng tiếp nhận những thông tin từ cảm biến để điểu khiển bánh xe và cánh tay. 
Trong ví dụ, chúng em chỉ xét dựa trên mức độ pin hiện tại robot nên quyết định tìm kiếm những lon soda như thế nào.
Tại mỗi thời điểm, robot có thể thực hiện một trong ba hành động: (1) thực hiện tìm kiếm một lon soda, (2) đứng yên và đợi người khác mang lon soda đến cho nó, (3) quay trở lại nơi sạc pin. 
Trạng thái của môi trường được xác định là lượng pin hiện tại của robot. 
Cách tốt nhất để tìm kiếm những lon soda là robot thực hiện hành động tìm kiếm, nhưng việc này sẽ làm giảm dung lượng của pin. 
Ngược lại nếu robot đứng yên và đợi thì dung lượng pin không giảm. 
Mỗi khi dung lượng pin ở mức thấp thì robot sẽ quay lại chỗ sạc pin. 
Trường hợp xấu nhất có thể xảy ra là robot không đủ dung lượng pin để quay lại nơi sạc; khi đó robot sẽ đứng yên và đợi ai đó mang nó đến chỗ sạc. 
Do đó robot cần có một chiến lược phù hợp để đạt được hiệu năng cao nhất có thể.
Hệ thống đưa ra những quyết định dựa trên mức năng lượng pin. 
Mức năng lượng này có thể được xác định hai mức \textit{cao} và \textit{thấp}. 
Khi đó tập trạng thái mà hệ thống có thể nhận được là $\mathcal{S} = \left \{\text{cao}, \text{thấp} \right \}$. 
Những hành động của hệ thống trong ví dụ này được xét đơn giản gồm ba hành động \textit{đợi}, \textit{tìm kiếm} và \textit{sạc pin}. Khi dung lượng pin ở trạng thái cao, hệ thống chỉ thực hiện hai hành động: tìm kiếm và đợi. 
Ngược lại khi ở trạng thái thấp, hệ thống có thể thực hiện ba hành động: tìm kiếm, đợi, và sạc pin.
$$\mathcal{A}(\text{cao}) =  \left \{\text{tìm kiếm}, \text{đợi} \right \}$$
$$\mathcal{A}(\text{thấp}) =  \left \{\text{tìm kiếm}, \text{đợi}, \text{sạc pin} \right \}$$

Khi mức năng lượng pin ở mức cao, nếu robot thực hiện tìm kiếm sẽ có xác suất $\alpha$ năng lượng pin vẫn ở mức cao, và $1 - \alpha$ năng lượng của pin sẽ chuyển về mức thấp. 
Mặt khác, khi mức năng lượng ở mức thấp, nếu robot thực hiện tìm kiếm sẽ có xác suất $\beta$ năng lượng pin ở mức thấp, và $1 - \beta$ chuyển đến mức cao (trường hợp này xảy ra khi dung lượng pin cạn kiệt và cần ai đó mang robot đến chỗ sạc). 
Ngoài ra, mỗi lon soda nhặt được tương ứng với $+1$ điểm thưởng và sẽ bị $-3$ điểm thưởng mỗi khi robot phải cần ai đó mang đến chỗ sạc. $r_{\text{đợi}}$, $r_{\text{tìm kiếm}}$ là số lượng lon soda kỳ vọng mà robot có thể thu gom được trong khi đợi và tìm kiếm. Hình \ref{graphRobot} minh họa cho mô hình MDP trong ví dụ robot thu gom lon soda.

%% Hình vẽ
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=6.5cm,>=stealth',bend angle=45 ,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=blue!20,minimum size = 15mm]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state] (high)                           {Cao};	
	\node [state] (low) [right of=high]              {Thấp};
	
	\node [action] (w1) [above = 1.5cm of high, label=left:Đợi] {}
	edge      					(high)
	edge [post,bend right] node[left]{$1, r_{\text{đợi}}$}      (high);
	
	\node [action] (s1) [below = 1.5cm of high, label=right:Tìm kiếm] {}
	edge        				(high)
	edge [post,bend left] node[left]{$\alpha, r_{\text{tìm kiếm}}$}       (high)
	edge [post,bend right] node[below=0.2]{$1 - \alpha, r_{\text{tìm kiếm}}$}       (low);
	
	\node [action] (s2) [above = 1.5cm of low, label=left:Tìm kiếm] {}
	edge        				(low)
	edge [post,bend left] node[right]{$\beta, r_{\text{tìm kiếm}}$}       (low)
	edge [post,bend right] node[above=0.2]{$1 - \beta, -3$}       (high);
	
	\node [action] (w2) [below = 1.5cm of low, label=right:Đợi] {}
	edge        				(low)
	edge [post,bend right] node[right]{$1, r_{\text{đợi}}$}       (low);
	
	\node [action] (re) [left = 1.5cm of low, , label=above:Sạc pin] {}
	edge						(low)
	edge [post] node[above]{$1, 0$} (high);
	\end{scope}		
	\end{tikzpicture}
	\caption[Đồ thị minh họa việc chuyển trạng thái cho robot thu gom]{Đồ thị minh họa việc chuyển trạng thái cho robot thu gom. 
	Trong đồ thị có hai loại node: node trạng thái và node hành động. 
	Node trạng thái minh họa những trạng thái mà hệ thống có thể nhận được và được ký hiệu là một vòng tròn lớn với tên của trạng thái bên trong. 
	Node hành động tương ứng với cặp (trạng thái, hành động). 
	Việc thực hiện hành động $a$ tại trạng thái $s$ tương ứng trên đồ thị là một cạnh bắt đầu từ node trạng thái $s$ tới node hành động $a$. 
	Khi đó môi trường sẽ trả ra trạng thái tiếp theo $s'$ ứng với đích của mũi tên đi từ node hành động $a$. 
	Giá trị được ký hiệu trên mỗi mũi tên lần lượt là xác suất chuyển tới trạng thái mới và giá trị điểm thưởng kỳ vọng nhận được.
	Ví dụ: khi mức năng lượng pin đang ở trạng thái \textit{thấp}, hệ thống quyết định thực hiện hành động \textit{sạc pin} thì trạng thái tiếp theo mà hệ thống nhận được sẽ là mức năng lượng pin \textit{cao} (với xác suất là 1) và giá trị kỳ vọng điểm thưởng là 0.}
	\label{graphRobot}
\end{figure}

%% ----------------------------------------------------

\subsection{Chính sách và hàm giá trị} \label{sec:policy_value}
Có thể nói chính sách như ``bộ não'' của hệ thống: chính sách quyết định cách thức mà hệ thống hành động trong những trạng thái cụ thể. 
Do đó, hệ thống có được một chính sách tốt đồng nghĩa với việc khả năng ra quyết định của hệ thống trở nên tốt và thông minh hơn.

Hàm giá trị cho biết những trạng thái hoặc những cặp hành động và trạng thái ``tốt'' đến mức thế nào. 
Khái niệm ``tốt'' ở đây nghĩa là giá trị kỳ vọng tổng điểm thưởng mà hệ thống có thể nhận được cao đến mức nào.
Hầu hết các thuật toán trong học tăng cường đều tập trung vào việc đánh giá những hàm giá trị và qua đó cải thiện chính sách trở nên tốt hơn \cite{sutton1998introduction}. 
Tổng điểm thưởng mà hệ thống có thể nhận được trong tương lai phụ thuộc vào cách thức chọn hành động. 
Do đó, hàm giá trị chịu ảnh hưởng rất nhiều vào chính sách. 
Giá trị của trạng thái $s$ dưới một chính sách $\pi$, ký hiệu $v_{\pi}(s)$, là kỳ vọng tổng điểm thưởng nhận được khi bắt đầu từ trạng thái $s$ và thực hiện theo chính sách $\pi$ sau đó. 
Với mô hình MDP, $v_{\pi}(s)$ được định nghĩa như sau:
\begin{equation}
v_{\pi}(s) = \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s\right ] = \mathbb{E}_{\pi}\left [\sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+1} \middle|\ \mathit{S}_t= s\right ]
\end{equation}
$v_{\pi}$ được gọi là hàm giá trị trạng thái dưới chính sách $\pi$.
Chính sách $\pi$ được ghi dưới ký hiệu kỳ vọng thể hiện việc hàm giá trị này phụ thuộc vào $\pi$.

Tương tự, chúng ta định nghĩa giá trị hành động $a$ tại trạng thái $s$, được ký hiệu $q_{\pi}(s,a)$, là giá trị kỳ vọng của tổng điểm thưởng khi thực hiện hành động $a$ trong trạng thái $s$ rồi sau đó chọn hành động theo chính sách $\pi$:
\begin{equation}
\label{action_value}
q_{\pi}(s,a) = \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s, \mathit{A}_{t} = a  \right ] = \mathbb{E}_{\pi}\left [\sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+1} \middle|\ \mathit{S}_t= s, \mathit{A}_{t} = a \right ]
\end{equation}
$q_{\pi}$ được gọi là hàm giá trị hành động dưới chính sách $\pi$.

Từ các định nghĩa trên, ta có thể tìm mối liên hệ giữa giá trị trạng thái và giá trị hành động.
Phương trình (\ref{eq:relation_value_action}) xác định giá trị của một trạng thái bằng tổng có trọng số của các giá trị hành động tại trạng thái đó (với trọng số là xác suất thực hiện hành động). 
Hình \ref{fig:relationship_value_functions}a minh họa quan hệ giữa giá trị của một trạng thái $s$ và giá trị của các hành động có thể thực hiện tại trạng thái đó. 
Hình \ref{fig:relationship_value_functions}b cho thấy từ việc thực hiện hành động $a$ tại trạng thái $s$, trạng thái tiếp theo $s'$ có thể khác nhau. 
Do đó giá trị của hành động $a$ ở trạng thái $s$ chính là kỳ vọng điểm thưởng nhận được ngay sau đó cộng với tổng giá trị kỳ vọng của các trạng thái tiếp theo đó đã được nhân với hệ số $\gamma$. 
Phương trình (\ref{eq:relation_action_value}) thể hiện mối quan hệ này giữa giá trị hành động và giá trị trạng thái.
\begin{align}
v_{\pi} = {} & \sum_{a \in \mathcal{A}(s)}^{}\pi(a \mid s)q_{\pi}(s,a) \label{eq:relation_value_action}\\
q_{\pi}(s,a) = {} & \mathcal{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{}\mathcal{P}_{ss'}^{a}v_{\pi}(s') \label{eq:relation_action_value}
\end{align}
%----------------------------------------	
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state, label={[name=label node]right:$s \mapsto v_{\pi}(s)$}] (s1)      {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action, label={[name=label node]right:$a \mapsto q_{\pi}(s,a)$}] (a1) [below right = 2 cm and 1 cm of s1]    {}
	edge node[right]{$r$} (s1);	
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1);
	
	\end{scope}		
	
	\begin{scope}[xshift=8cm]
	% Second net		
	\node [action, label={[name=label node]right:$s,a \mapsto q_{\pi}(s,a)$}] (a4)      {};		
	\node (label) [left = 2cm of a4] {(b)};		
	\node [state] (s8) [below right = 2 cm and 1 cm of a4, label=right:$s' \mapsto v_{\pi}(s')$]    {}
	edge node[right]{$r$} (a4);		
	\node [state] (s9) [below left  = 2 cm and 1 cm of a4]    {}
	edge (a4);		
	\end{scope}		
	\end{tikzpicture}		
	\caption[Đồ thị minh họa quan hệ giữa những hàm giá trị]{Đồ thị minh họa quan hệ giữa hàm giá trị trạng thái và hàm giá trị hành động.
	Node tròn rỗng ứng với trạng thái và node tròn tô đậm ứng với hành động tại trạng thái đó.
	Với hình (a), giá trị trạng thái $s$ là $v_{\pi}(s)$.
	Tại trạng thái này, hệ thống có thể thực hiện \textbf{một trong số} các hành động ở ngay bên dưới.
	Xác suất chọn hành động này được thể hiện qua chính sách $\pi$.
	Vì vậy, giá trị trạng thái $s$ chính là tổng có trọng số của giá trị từng hành động (với trọng số là xác suất thực hiện hành động đó).
	Với hình (b), giá trị hành động $a$ tại trạng thái $s$ là $q_{\pi}(s,a)$.
	Khi thực hiện hành động $a$, hệ thống nhận được điểm thưởng $r$ và di chuyển sang trạng thái mới.
	Trạng thái mới là ngẫu nhiên nên để tính giá trị hành động, ta phải tính tổng có trọng số giá trị của mọi trạng thái tiếp theo có thể có (với trọng số là xác suất đi đến trạng thái đó).}
	\label{fig:relationship_value_functions}
\end{figure}

Hàm giá trị có một tính chất cơ bản thường được áp dụng trong học tăng cường đó là mối quan hệ đệ quy. Cho bất kỳ chính sách $\pi$ với bất kỳ trạng thái $s$, giá trị của trạng thái đó được xác định bằng:	
\begin{align}
\label{bell_state}
v_{\pi}(s) = {}& \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s\right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^{2} \mathit{R}_{t+3} + ... \mid \mathit{S}_t = s  \right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma( {R}_{t+2} + \gamma \mathit{R}_{t+3} + ...) \mid \mathit{S}_t = s  \right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma\mathit{G}_{t + 1} \mid \mathit{S}_t = s  \right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma v_{\pi}(\mathit{S}_{t+1}) \mid \mathit{S}_t = s  \right ]
\end{align}	
Phương trình (\ref{bell_state}) được gọi là phương trình Bellman cho $v_{\pi}$. 
Từ phương trình này ta thấy được mối liên quan giữa giá trị của một trạng thái $s$ bất kỳ và giá trị của những trạng thái tiếp. 
Ý tưởng chính của phương trình Bellman đó là ``nhìn trước một bước'': đánh giá trạng thái hiện tại bằng cách nhìn trước tất cả những trạng thái tiếp theo có thể đạt được từ trạng thái đó.
Ý tưởng này được được minh họa trong hình \ref{backup_diagram}a. 
Từ một trạng thái, môi trường có thể trả ra trạng thái tiếp theo $s'$ khác nhau. 
Phương trình (\ref{bell_state}) sẽ trung bình tất cả các trường hợp có thể xảy ra lại theo xác suất mà chúng xuất hiện. 
Phương trình này cũng cho thấy giá trị của một trạng thái phải bằng kỳ vọng \textbf{điểm thưởng tức thì} cộng với \textbf{giá trị của những trạng thái tiếp theo}.
%----------------------------------------
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\label{fig_bellman_value}
	\node [state, label=above:$s$] (s1)                           {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action] (a1) [below = 2cm of s1]              {}
	edge (s1);
	
	\node [action] (a2) [left = 2cm of a1]              {}
	edge (s1);
	
	\node [action] (a3) [right = 2cm of a1, label=right:$a$]              {}
	edge (s1);
	
	\node [state] (s2) [below left = 2cm and 0.3cm of a1] {}
	edge (a1);
	
	\node [state] (s3) [below right = 2cm and 0.3cm of a1] {}
	edge (a1);
	
	\node [state] (s4) [below left = 2cm and 0.3cm of a2] {}
	edge (a2);
	
	\node [state] (s5) [below right = 2cm and 0.3cm of a2] {}
	edge (a2);
	
	\node [state] (s6) [below left = 2cm and 0.3cm of a3] {}
	edge (a3);
	
	\node [state] (s7) [below right = 2cm and 0.3cm of a3, label=right:$s'$] {}
	edge node[right]{$r$} (a3);	
	\end{scope}
	
	\begin{scope}[xshift=8cm]
	% Second net
	\label{fig_bellman_action}
	\node [action, label={[name=label node]above:$s,a$}] (a4)      {};
	
	\node (label) [left = 2cm of a4] {(b)};
	
	\node [state] (s8) [below right = 2 cm and 1 cm of a4, label=right:$s'$]    {}
	edge node[right]{$r$} (a4);	
	
	\node [state] (s9) [below left  = 2 cm and 1 cm of a4]    {}
	edge (a4);
	
	\node [action] (a5) [below left  = 2 cm and 0.3 cm of s8]    {}
	edge (s8);
	
	\node [action] (a6) [below right  = 2 cm and 0.3 cm of s8, label=right:$a'$]    {}
	edge (s8);
	
	\node [action] (a7) [below left  = 2 cm and 0.3 cm of s9]    {}
	edge (s9);
	
	\node [action] (a8) [below right  = 2 cm and 0.3 cm of s9]    {}
	edge (s9);	
	
	\end{scope}
	
	\end{tikzpicture}
	\caption[Đồ thị minh họa phương trình Bellman cho hàm giá trị]{Đồ thị minh họa phương trình Bellman cho (a) $v_{\pi}$ và (b) $q_{\pi}$.
	}
	\label{backup_diagram}
\end{figure}

\begin{equation}
\label{bell_action}
q_{\pi}(s,a) = \mathbb{E}_{\pi} \left[\mathit{R}_{t+1} + \gamma q_{\pi}(\mathit{S}_{t+1}, \mathit{A}_{t+1}) \mid \mathit{S}_t = s, \mathit{A}_t = a \right]
\end{equation}

Phân tích phương trình (\ref{action_value}) tương tự như đã làm đối với hàm giá trị trạng thái, ta có được phương trình (\ref{bell_action}).
Hình \ref{backup_diagram}b minh họa ý tưởng nhìn trước một bước để tính giá trị của một hành động ở trạng thái hiện tại. 
Thực hiện một hành động $a$ ở trạng thái $s$, trạng thái tiếp theo $s'$ có thể khác nhau và điểm thưởng $r$ cũng có thể khác nhau. 
Trong mỗi trạng thái $s'$ lại có nhiều hành động $a'$ khác nhau có thể thực hiện. 
Phương trình (\ref{bell_action}) sẽ trung bình tất cả các trường hợp có thể đó lại theo xác suất mà chúng được thực hiện. 
Hay nói cách khác, phương trình (\ref{bell_action}) cho thấy giá trị của một hành động $a$ tại trạng thái $s$, $q_{\pi}(a,s)$ được xác định bằng kỳ vọng điểm thưởng tức thì cộng với giá trị hành động tại trạng thái kế tiếp.

\subsection{Chính sách tối ưu và hàm giá trị tối ưu}
Để giải bài toán học tăng cường, chúng ta cần tìm một chính sách sao cho hệ thống có thể đạt được nhiều điểm thưởng nhất có thể. 
Một chính sách $\pi$ được xác định là tốt hơn hoặc bằng chính sách $\pi'$ khi với một trạng thái $s$ bất kỳ, giá trị của trạng thái này theo chính sách $\pi$ luôn lớn hơn hoặc bằng giá trị của trạng thái đó theo chính sách $\pi'$:
\begin{equation}
\pi \geq \pi' \Longleftrightarrow v_{\pi}(s) \geq v_{\pi'}(s), \forall s \in \mathcal{S}
\end{equation}

Luôn có ít nhất một chính sách tốt hơn hoặc bằng tất cả các chính sách còn lại \cite{sutton1998introduction}. 
Các chính sách này được gọi chung là \textit{chính sách tối ưu} và được ký hiệu $\pi_{*}$. 
Những chính sách tối ưu đều cùng có chung một hàm giá trị trạng thái và hàm giá trị hành động.
Do vậy, ta ký hiệu $v_{*}(s)$ và $q_{*}(s,a)$ là hàm giá trị tối ưu thay vì $v_{\pi_{*}}(s)$ và $q_{\pi_{*}}(s, a)$.
Hai loại hàm giá trị này có thể được gọi chung là \textit{hàm giá trị tối ưu}. 
Chúng ta cũng có thể gọi tách biệt \textit{hàm giá trị trạng thái tối ưu} và \textit{hàm giá trị hành động tối ưu}. 
Phương trình (\ref{eq:optimal_state}) và (\ref{eq:optimal_action}) định nghĩa hình thức cho hai loại hàm này
\begin{align}
v_{*}(s) = {} & \max_{\pi}v_{\pi}(s), \forall s \in \mathcal{S} \label{eq:optimal_state} \\
q_{*}(s,a) = {} & \max_{\pi}q_{\pi}(s,a), \forall s \in \mathcal{S} \text{ và } \forall a \in \mathcal{A}(s) \label{eq:optimal_action}
\end{align}
Từ hai phương trình (\ref{eq:optimal_state}) và (\ref{eq:optimal_action}), ta thấy rằng để xác định hàm giá trị tối ưu của mỗi trạng thái $s$ hoặc cặp trạng thái và hành động ($s,a$), ta cần thử đánh giá giá trị của chúng theo tất cả các chính sách có thể có và chọn giá trị cao nhất.

Hình \ref{fig:relationship_optimal_functions} minh hoạ quan hệ giữa hàm giá trị trạng thái tối ưu và hàm giá trị hành động tối ưu; khi có được hàm này ta dễ dàng có được hàm còn lại. 
Trong hình \ref{fig:relationship_optimal_functions}a, ta có thể xác định giá trị tối ưu cho trạng thái $s$ dựa trên giá trị hành động tối ưu. 
Phương trình (\ref{eq:optimal_state_action}) xác định giá trị tối ưu cho trạng thái $s$ bằng cách chọn giá trị hành động tối ưu lớn nhất trong các hành động có thể thực hiện. 
Tương tự trong hình \ref{fig:relationship_optimal_functions}b, ta có thể xác định giá trị tối ưu cho hành động $a$ ở trạng thái $s$ dựa trên hàm giá trị trạng thái tối ưu của các trạng thái kế tiếp. 
Phương trình (\ref{eq:optimal_action_state}) xác định giá trị tối ưu của hành động $a$ tại trạng thái $s$ gồm hai phần: kỳ vọng điểm thưởng nhận được tức thì và giá trị trạng thái tối ưu của trạng thái kế tiếp.

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state, label={[name=label node]right:$s \mapsto v_{*}(s)$}] (s1)      {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action, label={[name=label node]right:$a \mapsto q_{*}(s,a)$}] (a1) [below right = 2 cm and 1 cm of s1]    {}
	edge node[right]{$r$} (s1);	
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a2--s1--a1};
	
	\end{scope}		
	
	\begin{scope}[xshift=8cm]
	% Second net		
	\node [action, label={[name=label node]right:$s,a \mapsto q_{*}(s,a)$}] (a4)      {};		
	\node (label) [left = 2cm of a4] {(b)};		
	\node [state] (s8) [below right = 2 cm and 1 cm of a4, label=right:$s' \mapsto v_{*}(s')$]    {}
	edge node[right]{$r$} (a4);		
	\node [state] (s9) [below left  = 2 cm and 1 cm of a4]    {}
	edge (a4);		
	\end{scope}		
	\end{tikzpicture}		
	\caption[Đồ thị minh họa quan hệ giữa những hàm giá trị tối ưu]{Đồ thị minh hoạ quan hệ giữa hàm giá trị trạng thái tối ưu và hàm giá trị hành động tối ưu.
	Đồ thị bên trái thể hiện cách tính giá trị trạng thái tối ưu dựa vào giá trị hành động tối ưu.
	Đồ thị này cũng tương tự như giữa hàm giá trị trạng thái với hàm giá trị hành động.
	Khác biệt lúc này là có cung nối giữa các hành động thể hiện việc lấy $\max$ giá trị của các hành động.
	Đồ thị bên phải thể hiện cách tính giá trị hành động tối ưu dựa vào giá trị trạng thái tối ưu.
	}
	\label{fig:relationship_optimal_functions}
\end{figure}	
\begin{align}
%		q_{*}(s,a) = \mathbb{E} \left[ \mathit{R}_{t+1} + \gamma v_{*}(\mathit{S}_{t+1}) \mid \mathit{S}_{t} = s, \mathit{A}_{t} = a \right]
v_{*}(s) = {} & \max_{a \in \mathcal{A}(s)} q_{*}(s,a) \label{eq:optimal_state_action}\\
q_{*}(s,a) = {} & \mathit{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{} \mathcal{P}_{ss'}^{a} v_{*}(s') \label{eq:optimal_action_state} \\
v_{*}(s) = {} & \max_{a \in \mathcal{A}(s)} \mathit{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{} \mathcal{P}_{ss'}^{a} v_{*}(s') \label{eq:bellman_optimal_state}\\
q_{*}(s,a) = {} & \mathit{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{} \mathcal{P}_{ss'}^{a} \max_{a' \in \mathcal{A}(s')} q_{*}(s',a') \label{eq:bellman_optimal_action}		
\end{align}

Phương trình (\ref{eq:bellman_optimal_state}) và (\ref{eq:bellman_optimal_action}) dễ dàng có được bằng cách thay thế hai phương trình (\ref{eq:optimal_state_action}) và (\ref{eq:optimal_action_state}) qua lại lẫn nhau. 
Từ hai phương trình này, ta thấy được dạng phương trình Bellman trong hàm giá trị trạng thái tối ưu và hàm giá trị hành động tối ưu. 
Hình \ref{fig:bellman_optimal_function} minh họa ý tưởng nhìn trước một bước của phương trình Bellman trong hàm giá trị tối ưu. 
Trong đó hình \ref{fig:bellman_optimal_function}a minh họa cách thức xác định giá trị tối ưu cho một trạng thái ứng với phương trình (\ref{eq:bellman_optimal_state}). 
Hình \ref{fig:bellman_optimal_function}b minh họa cách thức xác định giá trị tối ưu của một hành động ở một trạng thái ứng với phương trình (\ref{eq:bellman_optimal_action}).
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	
	\node [state, label={[name=label node]right:$s \mapsto v_{*}(s)$}] (s1)      {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action] (a1) [below right = 2 cm and 1 cm of s1, label=right:$a$]    {}
	edge(s1);
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a2--s1--a1};
	
	\node [state] (s2) [below left  = 2 cm and 0.3 cm of a1]    {}
	edge (a1);
	
	\node [state, label={[name=label node]right:$s' \mapsto v_{*}(s')$}] (s3) [below right  = 2 cm and 0.3 cm of a1]    {}
	edge node[right]{$r$}	 (a1);
	
	\node [state] (s4) [below left  = 2 cm and 0.3 cm of a2]    {}
	edge (a2);
	
	\node [state] (s5) [below right  = 2 cm and 0.3 cm of a2]    {}
	edge (a2);	
	\end{scope}
	
	\begin{scope}[xshift=8cm]
	
	\node [action, label={[name=label node]right:$(s,a) \mapsto q_{*}(s,a)$}] (a3)      {};
	
	\node (label) [left = 2cm of a3] {(b)};
	
	\node [state] (s6) [below right = 2 cm and 1 cm of a3, label=right:$s'$]    {}
	edge node[right]{$r$} (a3);	
	
	\node [state] (s7) [below left  = 2 cm and 1 cm of a3]    {}
	edge (a3);
	
	\node [action] (a4) [below left  = 2 cm and 0.3 cm of s6]    {}
	edge (s6);
	
	\node [action, label={[name=label node]right:$a' \mapsto q_{*}(s',a')$}] (a5) [below right  = 2 cm and 0.3 cm of s6]    {}
	edge (s6)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a4--s6--a5};
	
	\node [action] (a6) [below left  = 2 cm and 0.3 cm of s7]    {}
	edge (s7);
	
	\node [action] (a7) [below right  = 2 cm and 0.3 cm of s7]    {}
	edge (s7)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a6--s7--a7};
	
	\end{scope}		
	\end{tikzpicture}
	\caption[Đồ thị minh họa phương trình Bellman trong hàm giá trị tối ưu]{Đồ thị minh họa phương trình Bellman trong (a) $v_{*}$} và (b) $q_{*}$
	\label{fig:bellman_optimal_function}
\end{figure}

\subsection{Quy trình tìm chính sách tối ưu}
Trong các bài toán học tăng cường, mục tiêu chính của ta là tìm được chính sách tối ưu $\pi_{*}$ nhằm giúp cho hệ thông giải quyết bài toán tốt nhất có thể. Do đó ta cần có quy trình để thay đổi chính sách hiện tại trở nên tối ưu. Quy trình này được gọi là \textit{quy trình tìm chính sách tối ưu}. Hình \ref{fig:policy_iteration} minh họa quy trình này được chia thành hai giai đoạn:
\begin{itemize}
	\item \textbf{Đánh giá chính sách}: Việc đánh giá một chính sách $\pi$ được thực hiện bằng cách xác định hàm giá trị trạng thái hoặc hàm giá trị hành động của dưới chính sách đó.
	\item \textbf{Cải thiện chính sách}: Sau khi có được hàm giá trị của một chính sách $\pi$, chính sách cải thiện mới $\pi'$ được tạo ra bằng cách thực hiện tham lam trên hàm giá trị của chính sách $\pi$, tức là chỉ chọn thực hiện hành động có giá trị cao nhất dựa trên hàm giá trị hành động có được. 
\end{itemize}
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=5.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=blue!20,minimum size = 15mm]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node (policy)                           {\large $\pi$};	
	
	\node (value) [right of=policy]              {\large $\mathcal{V}$}
	edge [post,bend left] node[below]{Cải thiện} (policy)
	edge [pre,bend right] node[above]{Đánh giá} (policy);	
	
	\end{scope}			
	\end{tikzpicture}
	\caption[Quy trình tìm chính sách tối ưu]{Quy trình tìm chính chính sách tối ưu}
	\label{fig:policy_iteration}
\end{figure}
Một chính sách $\pi_{1}$ được cải thiện từ chính sách $\pi_{0}$ dựa trên hàm giá trị trạng thái $v_{\pi_{0}}$. Khi có được chính sách $\pi_{1}$ ta có thể tính được hàm giá trị $v_{\pi_{1}}$ qua đó tiếp tục cải thiện để có được chính sách $\pi_{2}$. Quá trình này diễn ra cho đến khi đạt được chính sách tối ưu.
$$\pi_{0} \xrightarrow{\text{Đánh giá}} v_{\pi_{0}} \xrightarrow{\text{Cải thiện}} \pi_{1} \xrightarrow{\text{Đánh giá}} v_{\pi_{1}} \xrightarrow{\text{Cải thiện}} \pi_{2} \cdots \xrightarrow{\text{Cải thiện}} \pi_{*} \xrightarrow{\text{Đánh giá}} v_{\pi_{*}}$$

\subsubsection{Đánh giá chính sách bằng quy hoạch động (Dynamic Programming)}
Quy hoạch động thường được dùng để giải quyết các bài toán tối ưu mà dữ liệu có tính thứ tự, ví dụ như dữ liệu chuỗi hay dữ liệu thời gian. Một bài toán tối ưu có thể được giải quyết bằng quy hoạch động cần có hai đặc điểm:
\begin{itemize}
	\item Quy tặc tối ưu (Principle of Optimality): các bài toán có thể phân rã thành các bài toán con, và kết quả của bài toán con này đóng góp vào lời giải của bài toán gốc.
	\item Các bài toán con chồng lấn lên nhau và lặp lại nhiều lần: nhằm tận dụng lại kết quả của những bài toán con đã tính toán trước đó.
\end{itemize}

Trong nhiều bài toán học tăng cường, kỹ thuật quy hoạch động được dùng để tìm chính sách tối ưu hoặc tối ưu hàm giá trị. Để có thể áp dụng kỹ thuật quy hoạch động, những bài toán này cũng cần phải thỏa yêu cầu là hệ thống có kiến thức đầy đủ về môi trường hay cách khác môi trường có mô hình MDP.
Quy hoạch động xác định hàm giá trị của một chính sách bằng cách cập nhật hàm giá trị được khởi tạo bất kỳ ban đầu qua nhiều vòng lặp, dựa vào phương trình Bellman. Ý tưởng của cách xác định này như sau: Ban đầu khởi tạo hàm giá trị $v_0$ bất kỳ cho tất cả các trạng thái, trừ trạng thái kết thúc được luôn có giá trị là 0. Tiến hành cập nhật hàm giá trị mới $v_1$ cho chính sách dựa trên hàm giá trị $v_0$ theo phương trình \ref{eq:DP_Value_Update}. Tương tự cập nhật hàm giá trị mới $v_2$ dựa trên $v_1$. Quá trình lặp cho đến khi độ khác biệt giữa hàm giá trị sau và giá trị trước đó nhỏ hơn một lượng cho trước. Quy trình cập nhật được minh họa trong hình \ref{fig:Update_Value_DP}, trong đó giá trị mới $v_{k+1}$ của trạng thái $s$ được xác định dựa trên giá trị kỳ vọng điểm thưởng nhận được theo chính sách $\pi$, và giá trị hiện tại $v_{k}$ của các trạng thái $s'$ kế tiếp trạng thái $s$. Tổng thể của việc đánh giá chính sách bằng quy hoạch động được trình bày ở thuật toán \ref{alg_DP}
\begin{equation}
v_{k+1}(s) \leftarrow \sum_{a \in \mathcal{A}}^{}\pi(a \mid s)(\mathcal{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{}\mathcal{P}_{ss'}^{a}v_{k}(s'))
\label{eq:DP_Value_Update}
\end{equation}		
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	
	\node [state, label={[name=label node]right:$s \mapsto v_{k+1}(s)$}] (s1)      {};
	
	\node [action] (a1) [below right = 2 cm and 2 cm of s1, label=right:$a$]    {}
	edge(s1);
	
	\node [action] (a2) [below left  = 2 cm and 2 cm of s1]    {}
	edge (s1);
	
	\node [state] (s2) [below left  = 2 cm and 1 cm of a1]    {}
	edge (a1);
	
	\node [state, label={[name=label node]right:$s' \mapsto v_{k}(s')$}] (s3) [below right  = 2 cm and 1 cm of a1]    {}
	edge node[right]{$r$}	 (a1);
	
	\node [state] (s4) [below left  = 2 cm and 1 cm of a2]    {}
	edge (a2);
	
	\node [state] (s5) [below right  = 2 cm and 1 cm of a2]    {}
	edge (a2);	
	\end{scope}
	
	\end{tikzpicture}
	\caption[Cập nhật hàm giá trị bằng quy hoạch động]{Đồ thị minh họa cập nhật hàm giá trị bằng quy hoạch động}
	\label{fig:Update_Value_DP}
\end{figure}

\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị bằng quy hoạch động}
	\label{alg_DP}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\State Khởi tạo ngẫu nhiên $V(s)$ cho tất cả trạng thái $s$ không phải trạng thái kết thúc. Nếu $s$ là trạng thái kết thúc, $V(s) = 0$
		\Repeat
		\State $\Delta \leftarrow 0$ \%\% Tính độ khác biệt giữa hàm giá trị cũ và giá trị mới. Độ lớn của $\Delta$ được xác định là độ khác biệt lớn nhất giữa giá trị cũ và giá trị mới của một trạng thái trong tất cả các trạng thái.
		\For{$s \in \mathcal{S}$} \%\% Với mỗi trạng thái
		\State $v \leftarrow  V(s)$ \%\% Lưu giá trị hiện tại của trạng thái $s$
		\State $V(s) \leftarrow \sum_{a \in \mathcal{A}}^{}\pi(a \mid s)(\mathcal{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{}\mathcal{P}_{ss'}^{a}V(s'))$ \%\% Tính giá trị mới cho trạng thái $s$ dựa trên giá trị hiện tại của các trạng thái $s'$ kế tiếp của trạng thái $s$, và giá trị kỳ vọng của các hành động tại trạng thái đó theo chính sách $\pi$.
		\State $\Delta \leftarrow \max(\Delta, \left |v - V(s) \right |)$ \%\% Cập nhật giá trị mới cho $\Delta$
		\EndFor
		\Until $\Delta < \theta$ (Một lượng đủ nhỏ) 
	\end{algorithmic}
\end{algorithm}

Mặc dù đã quy hoạch động đã được chứng minh là xấp xỉ tốt hay thậm chí là tìm được hàm giá trị trạng thái của chính sách $\pi$ \cite{gordon1995stable}, nhưng trong các bài toán học thực tế của học tăng cường đặc biệt là những bài toán lớn thì quy hoạch động trở nên không khả thi do chi phí tính toán cao, trong trường hơp xấu nhất chi phí tính toán thuộc $O(k^{n})$ với $k$ là số hành động và $n$ là số trạng thái.

\subsubsection{Cải thiện chính sách bằng phương pháp tham lam (greedy)}
Mục tiêu chúng ta xác định hàm giá trị cho một chính sách là để tìm một chính sách tốt hơn chính sách hiện tại. Giả sử chúng ta có một chính sách $\pi$ cố định tức là với mỗi trạng thái $s$, chính sách này luôn chọn thực hiện một hành động cố định, $\pi(s) = a$. Và cũng đã xác định được một hàm giá trị $v_{\pi}$ cho một chính sách đó. Với một trạng thái $s$, câu hỏi đặt ra là chúng ta nên thay đổi một chính sách cố định khác chọn hành động $a' \neq \pi(s)$ không? Chúng ta biết giá trị của trạng thái $s$ theo chính sách hiện tại $\pi$, $v_{\pi}(s)$, tốt như thế nào nhưng liệu chính sách mới $\pi'$ có tốt hơn hay trở nên tệ đi? Theo \cite{sutton1998introduction}, ta có thể cải thiện một chính sách bằng cách chọn hành động có giá trị cao nhật tại mỗi trạng thái $s$. Chính xách mới $\pi'$ được xác định trong \ref{eq:update_policy}.
%\begin{equation}
%\label{eq:update_policy}
%\pi'(s) = \underset{a \in \mathcal{A}}{\operatorname{argmax}}q_{\pi}(s,a)
%\end{equation}
\begin{equation}
\label{eq:update_policy}
\pi'(a|s)
\left\{\begin{matrix}
1 & \text{nếu }a = \underset{a \in A}{\operatorname{argmax}}q(s,a)\\ 
0 & \text{ngược lại}
\end{matrix}\right.
\end{equation}
Ngoài ra, việc cải thiện chính sách bằng phương pháp tham lam này, ta đồng thời cũng cải thiện được hàm giá trị \cite{sutton1998introduction}.
%\begin{align}
%v_{\pi}(s) \leqslant {} & q_{\pi'}(s, \pi'(s)) = \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma v_{\pi}(\mathit{S}_{t+1}) \mid \mathit{S}_t = s\right] \notag \\
%\leqslant {} & \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma q_{\pi}(\mathit{S}_{t+1}, \pi'(\mathit{S_{t+1}})) \mid \mathit{S}_t = s\right] \notag \\
%\leqslant {} & \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^2 q_{\pi}(\mathit{S}_{t+2}, \pi'(\mathit{S}_{t+2})) \mid \mathit{S}_t = s\right] \notag \\
%\leqslant {} & \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \dots \mid \mathit{S}_t = s\right] = v_{\pi'}(s) \label{eq:improve_value_function}
%\end{align}
Việc đánh giá và cải thiện chính sách qua nhiều vòng lặp sẽ hội tụ về chính sách tối ưu cũng như hàm giá trị tối ưu. Hình \ref{fig:policy_iteration_MDP} minh họa quá trình hội tụ của lặp chính sách.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{policy_iteration}
	\caption[Hội tụ của quy trình tìm kiếm chính sách tối ưu]{Ta có một chính $\pi$, đầu tiên ta thực hiện đánh giá chính sách $\pi$ để có được hàm giá trị theo chính sách này. Khi có được hàm giá trị, ta thực hiện cải thiện chính sách bằng cách lựu chọn tham lam hành động có giá trị lớn nhất dựa trên hàm giá trị đang có. Sau khi có được chính sách mới, ta tiếp tục đánh giá chính sách để có được hàm giá trị theo chính sách đó. Và tiếp tục cải thiện khi đã có được hàm giá trị. Quá trình này được lặp nhiều lần cho đến khi đạt được chính sách tối ưu cũng như hàm giá trị tối ưu.}
	\label{fig:policy_iteration_MDP}	
\end{figure}

\section{Quy trình tìm chính sách tối ưu trong bài toán thực tế}
Trong nhiều bài toán thực tế thông thường chúng ta không có kiến thức đầy đủ về môi trường như ma trận chuyển trạng thái $\mathcal{P}$, ma trận điểm thưởng $\mathcal{R}$, tập các trạng thái $\mathcal{A}$. Do đó một yêu cầu được đặt ra là hệ thống phải có khả năng học từ những thông tin mà nó tiếp nhận được qua việc tương tác với môi trường. Các thông tin này thường ở dạng chuỗi (trạng thái, hành động, điểm thưởng)
$\mathit{S}_1, \mathit{A}_1, \mathit{R}_2, \mathit{S}_2, \mathit{A}_2, \mathit{R}_3, \dots, \mathit{S}_T$. Với $T$ là thời điểm kết thúc việc tương tác của hệ thống với môi trường.
%	\begin{itemize}
%		\item Dẫn nhập: Trên thực tế ta không có thông tin về môi trường
%		\item Qui trình đánh giá chính sách
%			\begin{itemize}
%				\item[+] Dựa trên hàm giá trị trạng thái: Monte-Carlo, TD(0), n-step TD, TD($\lambda$)
%				\item[+] Dựa trên hàm giá trị hành động: Monte-Carlo, Sarsa(0), n-step Sarsa, Sarsa($\lambda$)
%			\end{itemize}
%		\item Qui trình cải thiện chính sách: Phương pháp greedy
%	\end{itemize}

%	Chúng ta vừa định nghĩa hàm giá trị và chính sách tối ưu. Một hệ thống khi có được chính sách tối ưu thì làm việc rất tốt. Nhưng trong thực thế điều này hiếm khi xảy ra, thông thường để có được hàm giá trị hoặc chính sách tối ưu yêu cầu chi phí tính toán là rất lớn. Thậm chí, dù ta biết được mô hình hoàn toàn của môi trường, chúng ta vẫn không thể đạt được chính sách tối ưu bằng cách giải phương trình Bellman của hàm giá trị tối ưu. Do đó ta chỉ có thể xấp xỉ chính sách hoặc hàm giá trí tối ưu bằng những phương pháp xấp xỉ với các mức độ khác nhau. Ví dụ cụ thể cho vấn đề này đó là cờ vây, mặc dù số trạng thái và hành động có thể đều có thể tính toán được, nhưng chi phí tính toán để tìm một chính sách đánh cờ tối ưu là rất lớn. Do đó người chơi chỉ có thể dựa trên kinh nghiệm có để đi nước cờ tốt nhất cho mình.	Một vấn đề khác mà chúng ta phải đối mặt đó là khả năng lưu trữ có hạn. Thông thường để lưu tất cả các trạng thái hoặc các cặp trạng thái và hành động trong các bài toán học tăng cường  yêu cầu bộ nhớ rất lớn. Mặt khác, trong thực tế, chúng ta thường không biết đầy đủ về môi trường như những trạng thái có thể có, điểm thưởng của một trạng thái bất kỳ, và ma trận chuyển trạng thái. Do đó, ta cần những phương pháp có thể học từ những

\subsection{Phương pháp đánh giá chính sách}
Trong phần này, chúng em sẽ trình bày một số phương pháp được áp dụng để đánh giá chính sách.


\subsubsection{Phương pháp Monte Carlo (MC)}
Tương tự với quy hoạch động, Monte Carlo (MC) xác định hàm giá trị của một chính sách bằng cách cập nhật hàm giá trị khởi tạo qua nhiều vòng lặp. Điểm biệt khác với quy hoạch động của phương pháp MC là nó có thể áp dụng để đánh giá chính sách khi hệ thông không có kiến thức đầy đủ về môi trường. MC dựa trên những thông tin mà hệ thông có được qua việc tương tác với môi trường để xấp xỉ hàm giá trị. Thông thường những thông tin này được chia thành các \textit{mẫu thực nghiệm}. Mỗi mẫu thực nghiệm là một chuỗi bắt đầu từ một trạng thái bất kỳ cho đến khi đạt được một trong những trạng thái kết thúc. Khi đó MC chỉ thực hiện cập nhật hàm giá trị khi kết thúc một mẫu thực nghiệm.

Ý tưởng của MC là xác định giá trị của một trạng thái $s$ qua các mẫu thực nghiệm có sự xuất hiện trạng thái đó. Phương pháp MC xác định giá trị của trạng thái $s$ bằng cách trung bình các tổng điểm thưởng mà hệ thông nhận được sau khi quan sát được trạng thái $s$. Khi quan sát càng nhiều mẫu thực nghiệm có trạng thái $s$ xuất hiện, giá trị trung bình sẽ càng xấp xỉ tốt giá trị thực của trạng thái này theo chính sách $\pi$.

Một mẫu thực nghiệm là những thông tin có được trong quá trình hệ thống tương tác với môi trường bằng chính sách $\pi$. Giá trị của trạng thái $s$, $v(s)$, được tính dựa trên những mẫu thực nghiệm có trạng thái $s$ xuất hiện. Một trạng thái $s$ có thể xuất hiện nhiều lần trong một mẫu thực nghiệm. Lần xuất hiện đầu tiên của trạng thái $s$ trong một mẫu thực nghiệm được gọi là first-visit trạng thái đó. Phương pháp first-visit MC xác định giá trị trạng thái $s$, $v_{\pi}(s)$, bằng trung bình tất cả các tổng điểm thưởng mà hệ thống nhận sau lần first-visit của trạng thái $s$ trong các mẫu thực nghiệm. Tổng thể của việc đánh giá chính sách bằng first-visit MC được trình bày ở thuật toán \ref{alg_MC}. Hình \ref{fig:first_visit_MC} minh họa cách thức cập nhật hàm giá trị trên một mẫu thực nghiệm.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị trạng thái bằng phương pháp first-visit MC}
	\label{alg_MC}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\State Khởi tạo ngẫu nhiên $V(s)$ cho tất cả trạng thái $s$ không phải trạng thái kết thúc. Nếu $s$ là trạng thái kết thúc, $V(s) = 0$
		\State Khởi tạo danh sách rỗng \textbf{Returns($s$)} cho tất cả trạng thái $s \in \mathcal{S}$ \%\% Danh sách Returns($s$) chứa tất cả các tổng điểm thưởng mà hệ thống nhận được sau lần first-visit của trạng thái $s$ trong các mẫu thực nghiệm.
		\Repeat
		\State Tạo một mẫu thực nghiệm $E$ bằng chính sách $\pi$
		\For{mỗi trạng thái $s$ xuất hiện lần đầu trong $E$}
		\State $G \leftarrow$ tổng điểm thưởng nhận được sau lần xuất hiện đầu tiên của $s$
		\State Thêm $G$ vào danh sách Returns(s)
		\State $V(s) \leftarrow$ average(Returns(s))
		\EndFor
		\Until Thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 10mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	\tikzstyle{terminal}=[square,draw=black!60,
	fill=black!75,minimum size=10mm]
	
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state, label={[name=label node]below:$\mathit{S}_0$}] (s1)                           {};
	
	\node [action] (a1) [right = 0.7cm of s1]{}
	edge (s1);
	
	\node[state, fill=green, label={[name=label node]below:$\mathit{S}_1$}] (s2)  [right = 0.7cm of a1]{}
	edge node[above]{$r_1$} (a1);
	
	\node [action] (a2) [right = 0.7cm of s2]{}
	edge (s2);
	
	\node[state, fill=red, label={[name=label node]below:$\mathit{S}_2$}] (s3)  [right = 0.7cm of a2]{}
	edge node[above]{$r_2$} (a2);
	
	\node [action] (a3) [right = 0.7cm of s3]{}
	edge (s3);
	
	\node (etc) [right = 0.7cm of a3] {\large $\dots$};
	
	\node [state, fill=green, label={[name=label node]below:$\mathit{S}_{T-1}$}] (s4) [right = 0.7cm of etc]{};
	
	\node [action] (a4) [right = 0.7cm of s4]{}
	edge (s4);
	
	\node [state, fill = gray, label={[name=label node]below:Trạng thái kết thúc}] (t) [right = 0.7cm of a4]{}
	edge node[above]{$r_{T}$} (a4)
	edge [post,bend right] (s3)
	edge [post,bend right] (s2)
	edge [post,bend right] node[above]{Cập nhật giá trị} (s1);
	
	\end{scope}	
	\end{tikzpicture}
	\caption[Cập nhật hàm giá trị bằng phương pháp Monte Carlo]{Đồ thị minh họa cập nhật hàm giá trị trên một mẫu thực nghiệm bằng phương pháp first-visit MC. Hình tròn lớn ký hiệu cho trạng thái xuất hiện. Hình tròn nhỏ ký hiệu cho hành động thực hiện. Màu xác khác nhau giữa các hình tròn biểu thị cho sự khác nhau giữa các trạng thái. Phương pháp first-visit MC chỉ cập nhật giá trị cho các trạng thái khi kết thúc một mẫu thực nghiệm, và mỗi trạng thái chỉ được cập nhật một lần mặc dù trạng thái đó có thể xuất hiện nhiều lần trong cùng một mẫu.}
	\label{fig:first_visit_MC}
\end{figure}

Trong nhiều trường hợp, hệ thống không có được mô hình của môi trường, việc sử dụng hàm giá trị hành động trở nên khả thi hơn hàm giá trị trạng thái. Với việc có được mô hình của môi trường, hàm giá trị trạng thái là đủ để cải thiện một chính xách trở nên tốt hơn; nó đơn giản là nhìn trước trạng thái tiếp theo có thể đến và chọn bất kỳ hành động nào dẫn đến trạng thái đó mà đạt được nhiều điểm thưởng nhất. Ngược lại, nếu không có được mô hình của môi trường, hàm giá trị trạng thái là không đủ do hệ thống không thể xác định được trạng thái tiếp theo là trạng thái gì. Vì vậy, nó cần đánh giá giá trị của mỗi hành động trong mỗi trạng thái để xác định hành động nào nên thực hiện ở mỗi trạng thái qua đó cải thiện chính sách đang thực hiện.
Việc xác định hàm giá trị hành động $q_{\pi}$ được thực hiện tương tự như đã làm với hàm giá trị trạng thái $v_{\pi}$. Để xác định giá trị của hành động $a$ tại trạng thái $s$, nó thực hiện tính trung bình các tổng điểm thưởng mà hệ thộng nhận được dựa vào các mẫu thực nghiệm có sự xuất hiện của cặp trạng thái, hành động $(s,a)$. Lần xuất hiện đầu tiên của cặp trạng thái và hành động $(s,a)$ trong một mẫu thực nghiệm được gọi là first-visit của cặp trạng thái và hành động đó. Phương pháp first-visit MC xác định giá trị của hành động $a$ ở trạng thái $s$, $q_{\pi}(s,a)$, bằng trung bình tất cả các tổng điểm thưởng nhận được sau lần first-visit của cặp $(s,a)$ trong các mẫu thực nghiệm. Thuật toán \ref{alg_MC_action} trình bày cách thức xác định hàm giá trị hành động bằng first-visit MC.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị hành động bằng phương pháp first-visit MC}
	\label{alg_MC_action}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\State Khởi tạo ngẫu nhiên $Q(s,a)$ cho tất cả các cặp trạng thái, hành động $s,a$.
		\State Khởi tạo danh sách rỗng \textbf{Returns($s,a$)} cho tất cả các cặp trạng thái, hành động ($s,a$). \%\% Danh sách Returns($s,a$) chứa tất cả các tổng điểm thưởng mà hệ thống nhận được sau lần first-visit của cặp trạng thái, hành động ($s,a$) trong các mẫu thực nghiệm.
		\Repeat
		\State Tạo một thực nghiệm $E$ bằng chính sách $\pi$
		\For{mỗi cặp trạng thái, hành động $(s,a)$ xuất hiện lần đầu trong $E$}
		\State $G \leftarrow$ tổng điểm thưởng nhận được sau lần xuất hiện đầu tiên của cặp trạng thái, hành động $(s,a)$
		\State Thêm $G$ vào danh sách Returns($s,a$)
		\State $Q(s,a) \leftarrow$ average(Returns($s$))
		\EndFor
		\Until Thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

\subsubsection{Phương pháp Temporal Difference (TD)}
Phương pháp Temporal Difference (TD) kết hợp ý tưởng giữa Monte Carlo và quy hoạch động. Giống như Monte Carlo, phương pháp TD có thể học trực tiếp từ các mẫu thực nghiệm có được qua việc tương tác của hệ thống với môi trường mà không cần có mô hình của môi trường. Mặt khác tương tự với quy hoạch động, phương pháp TD thựa hiện cập nhật giá trị dựa trên những phần đã được xác định trước đó mà không phải đợi đến khi kết thúc một mẫu thực nghiệm như MC.

Giả sử ta có các trung bình $\mu_{1}$, $\mu_{2}$, ... của chuỗi $x_1, x_2, ...$ có thể được tính như sau:
\begin{align}
\mu_{k} = {}& \frac{1}{k}\sum_{j=1}^{k}x_{j} \notag \\
= {}& \frac{1}{k} \left(x_k + \sum_{j = 1}^{k - 1}x_j\right) \notag \\
= {}& \frac{1}{k}(x_k + \left(k - 1\right)\mu_{k-1}) \notag \\
= {}& \mu_{k - 1} + \frac{1}{k}(x_k - \mu_{k-1}) \label{eq:mean}
\end{align}
Phương pháp Monte Carlo phải đợi cho đến khi xác định được tổng điểm tưởng sau lần xuất hiện của một trạng thái để thực hiện cập nhật giá trị cho trạng thái đó, và giá trị của một trạng thái được cập nhật qua nhiều vòng lặp. Dựa vào phương trình \ref{eq:mean} giá trị của mỗi trạng thái có thể được cập nhật như sau:
\begin{align}
N(\mathit{S}_{t}) & \leftarrow N(\mathit{S}_{t}) + 1 \\
V(\mathit{S}_t) & \leftarrow V(\mathit{S}_t) + \frac{1}{N(\mathit{S}_{t})}(\mathit{G_t} - V(\mathit{S}_t)) \label{eq:update_value_MC}
\end{align}
Khi đó $\mathit{G}_t$ được gọi là mục tiêu cập nhật cho $V(S_t)$. Mặt khác, khi môi trường không ổn định việc cập nhật giá trị trạng thái theo \ref{eq:update_value_MC}  thường được cố định bằng hệ số $\alpha$:
\begin{equation}
V(\mathit{S}_t) \leftarrow V(\mathit{S}_t) + \alpha(\mathit{G_t} - V(\mathit{S}_t))
\end{equation}
Khác với phương pháp MC, phương pháp TD chỉ cần đợi tới bước tiếp theo ngay sau đó $t+1$ để hình thành một cái đích cho việc cập nhật qua việc quan sát điểm thưởng $R_{t+1}$ và giá trị của trạng thái tiếp theo $V(\mathit{S}_{t+1})$. Phương pháp TD đơn giản nhất được gọi là TD(0). Cách thức cập nhật giá trị của một trạng thái trong phương pháp này như sau:
\begin{equation}
V(\mathit{S}_t) \leftarrow 	V(\mathit{S}_t) + \alpha[\mathit{R}_{t+1} + \gamma V(\mathit{S}_{t+1}) - V(\mathit{S}_t)]
\label{eq:update_value_TD}
\end{equation}
Trong \ref{eq:update_value_TD} ta thấy mục tiêu cập nhật cho $V(\mathit{S}_t)$ trong TD(0) là $\mathit{R}_{t+1} + \gamma V(\mathit{S}_{t+1})$. Vì phương pháp TD thực hiện cập nhật giá trị của một trạng thái dựa một phần vào các giá trị của những trạng thái tiếp theo nên phương pháp này là một phương pháp "bootstapping", tương tự với quy hoạch động. Như đã định nghĩa trong \ref{sec:policy_value}, giá trị của trạng thái $s$ dưới chính sách $\pi$ được xác định:
\begin{align}
v_{\pi}(s) = {} & \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s\right ] \label{eq:value_base_return} \\
={} & \mathbb{E}_{\pi}\left [\sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+1} \middle|\ \mathit{S}_t= s\right ] \notag \\
= {} & \mathbb{E}_{\pi}\left [\mathit{R}_{t+1} + \gamma \sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+2} \middle|\ \mathit{S}_t= s\right ] \notag \\
= {} & \mathbb{E}_{\pi} \left[\mathit{R}_{t+1} + \gamma v_{\pi}(\mathit{S}_{t+1}) \mid \mathit{S}_t = s\right] \label{eq:value_base_bootstrap}
\end{align}
Qua đó, ta thấy rằng phương pháp MC sử dụng ước lượng của \ref{eq:value_base_return} là mục tiêu cập nhật; trong khi đó phương pháp quy hoạch động sử dụng ước lượng của \ref{eq:value_base_bootstrap}. Mục tiêu cập nhật của MC là một ước lượng vì không biết giá trị kỳ vọng trong \ref{eq:value_base_return} do đó tổng điểm thưởng trên một mẫu được sử dụng để thay thế cho giá trị kỳ vọng thực sự của nó. Mục tiêu cập nhật của quy hoạch động cũng là một ước lượng không phải vì giá trị kỳ vọng do trong quy hoạch động chúng ta đã giả định hệ thống có mô hình của môi trường; nhưng là vì $v_{\pi}(\mathit{S}_{t+1})$ là không biết do hiện tại nó đang được đánh giá; do đó $V(\mathit{S}_{t+1})$ được sử dụng để thay thế. Mục tiêu cập nhất trong TD là một ước lượng do cả hai nguyên nhân trên nên TD ước lượng giá trị kỳ vọng trong \ref{eq:value_base_return} qua mẫu và sử dụng ước lượng của hàm giá trị hiện tại $V$ để thay thế cho hàm giá trị đúng $v_{\pi}$. Vì vậy, phương pháp TD được cho là phương pháp kết hợp cách lấy mẫu của MC và bootstrapping của quy hoạch động. Từng bước thực hiện cập nhật hàm giá trị bằng phương pháp TD(0) được trình bày trong thuật toán thuật toán \ref{alg_TD_Zero}.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị trạng thái bằng TD(0)}
	\label{alg_TD_Zero}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\Repeat
		\State Tạo một mẫu thực nghiệm $E$ bằng chính sách $\pi$
		\State Khởi tạo trạng thái $s$
		\For{mỗi bước trong $E$}
		\State $A \leftarrow$ hành động được chọn theo chính $\pi$ tại $s$
		\State Thực hiện hành động $A$; quan sát điểm thưởng $r$ nhận được, và trạng thái tiếp theo $s'$
		\State $V(s) \leftarrow V(s) + \alpha \left[r + \gamma V(s') - V(s)\right]$ \%\% Thực hiện cập nhật giá trị cho trạng thái $s$
		\State $s \leftarrow s'$
		\EndFor
		\Until thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

Với bất kỳ chính sách $\pi$ cố định. Việc xác định hàm giá trị bằng phương pháp TD đã được chứng minh hội tu về hàm giá trị $v_{\pi}$ theo luật số lớn. Trong thực nghiệm, phương pháp TD thường hội tụ về hàm giá trị $v_{\pi}$ nhanh hơn phương pháp MC.

Phương pháp TD(0) xem giá trị của các trạng thái kế tiếp từ một trạng thái $s$ là giá trị đại diện cho lượng điểm thưởng mà trạng thái $s$ có thể nhận được ở tương lai; và dựa vào giá trị đại diện này và điểm thưởng nhận được ngay trạng thái $s$ để xác định giá trị của trạng thái đó. Tổng quát cho phương pháp TD là n-step TD. Để xác định giá trị của một trạng thái $s$, phương pháp n-step TD xem giá trị của trạng thái thứ $n$ sau đó là giá trị đại diện cho lượng điểm thưởng mà hệ thống có thể nhận được từ bước thứ $n$ trở về sau; và xác định giá trị của trạng thái $s$ dựa trên giá trị đại diện này cùng với điểm thưởng đã nhận được ở $n$ bước sáu đó. Hình \ref{fig:n_TD} minh họa cách xác định hàm giá trị trạng thái bằng phương pháp $n$-step TD. Phương pháp này thực hiện cập nhật cho một trạng thái ở bước thứ $n$ sau khi trạng thái $s$ xuất hiện trong mẫu thực nghiệm. 
\begin{figure}
	\centering
	\begin{tikzpicture}[>=stealth,bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node (label1) [align=left]  {TD($1$-step)};
	
	\node [state, label=left:$V(\mathit{S}_t)$] (s1) [below = 0.3cm of label1] {};
	
	\node [action] (a1) [below = 0.5cm of s1] {}
	edge  (s1);
	
	\node [state, label={[name=label node]below:$V(\mathit{S}_{t+1})$}] (s2) [below = 0.5 of a1] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a1);
	
	%--------------------------------
	\node (label2) [right = 1cm of label1] {2-step};
	
	\node [state] (s3) [below = 0.3cm of label2] {};
	
	\node [action] (a2) [below = 0.5cm of s3] {}
	edge (s3);
	
	\node [state] (s4) [below = 0.5 of a2] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a2);
	
	\node [action] (a3) [below = 0.5cm of s4] {}
	edge (s4);
	
	\node [state] (s5) [below = 0.5 of a3, label=below:$V(\mathit{S}_{t+2})$] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a3);
	
	%--------------------------------
	\node (label3) [right = 1cm of label2] {3-step};
	
	\node [state] (s6) [below = 0.3cm of label3] {};
	
	\node [action] (a4) [below = 0.5cm of s6] {}
	edge (s6);
	
	\node [state] (s7) [below = 0.5 of a4] {}
	edge  node[left]{$\mathit{R}_{t+1}$} (a4);
	
	\node [action] (a5) [below = 0.5cm of s7] {}
	edge (s7);
	
	\node [state] (s8) [below = 0.5 of a5] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a5);
	
	\node [action] (a6) [below = 0.5cm of s8] {}
	edge (s8);
	
	\node [state, label=below:$V(\mathit{S}_{t+3})$] (s9) [below = 0.5 of a6] {}
	edge node[left]{$\mathit{R}_{t+3}$} (a6);
	
	%--------------------------------
	\node (labeln) [right = 2cm of label3] {$n$-step};
	
	\node [state] (s10) [below = 0.3cm of labeln] {};
	
	\node [action] (a7) [below = 0.5cm of s10] {}
	edge (s10);
	
	\node [state] (s11) [below = 0.5 of a7] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a7);
	
	\node (dot2) [left = 1 of s11] {\Huge \dots};
	
	\node [action] (a8) [below = 0.5cm of s11] {}
	edge (s11);
	
	\node [state] (s12) [below = 0.5 of a8] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a8);	
	
	\node [action] (a9) [below = 0.5cm of s12] {}
	edge (s12);
	
	\node (dot1) [below = 0.5 of a9] {\huge \vdots};
	
	\node [action] (a10) [below = 0.5cm of dot1] {};
	
	\node [state, label=below:$V(\mathit{S}_{t+n})$] (s12) [below = 0.5 of a10] {}
	edge node[left]{$\mathit{R}_{t+n}$} (a10);
	
	%--------------------------------
	\node (labelMC) [right = 2cm of labeln] {Monte Carlo};
	
	\node [state] (s13) [below = 0.3cm of labelMC] {};
	
	\node [action] (a11) [below = 0.5cm of s13] {}
	edge (s13);
	
	\node [state] (s14) [below = 0.5 of a11] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a11);
	
	\node (dot3) [left = 1 of s14] {\Huge \dots};
	
	\node [action] (a12) [below = 0.5cm of s14] {}
	edge (s14);
	
	\node [state] (s15) [below = 0.5 of a12] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a12);
	
	
	\node [action] (a13) [below = 0.5cm of s15] {}
	edge (s15);
	
	\node [state] (s16) [below = 0.5 of a13] {}
	edge node[left]{$\mathit{R}_{t+3}$} (a13);
	
	\node [action] (a14) [below = 0.5cm of s16] {}
	edge (s16);
	
	\node [state] (s17) [below = 0.5 of a14] {}
	edge node[left]{$\mathit{R}_{t+4}$} (a14);
	
	\node [action] (a15) [below = 0.5cm of s17] {}
	edge (s17);
	
	\node [state] (s18) [below = 0.5 of a15] {}
	edge node[left]{$\mathit{R}_{t+5}$} (a15);
	
	\node [action] (a16) [below = 0.5cm of s18] {}
	edge (s18);
	
	\node (dot4) [below = 0.5cm of a16] {\huge \vdots};
	
	\node [action] (a17) [below = 0.5 of dot4] {};
	
	\node [state, fill = gray, label={[name=label node]below:Trạng thái kết thúc}] (s19) [below = 0.5 of a17] {}
	edge node[left]{$\mathit{R}_{T}$} (a17);
	
	\end{scope}	
	\end{tikzpicture}
	\caption[Minh họa phương pháp $n$-step TD]{Đồ thị bên trái ngoài cùng minh họa xác định hàm giá trị bằng phương pháp TD(0), trong khi đó đồ thị bên phải ngoài cùng minh họa cho phương pháp Monte Carlo. Các đồ thị ở giữa minh họa phương pháp $n$-step TD ứng với từng giá trị của $n$}
	\label{fig:n_TD}
\end{figure}

Xét một chuỗi trạng thái, điểm thưởng $\mathit{S}_{t}, \mathit{R}_{t+1}, \mathit{S}_{t+1}, \mathit{R}_{t+2}, \dots, \mathit{S}_{T}$. Như chúng ta đã biết, Monte Carlo thực hiện cập nhật ước lượng giá trị của trạng thái $s$ chỉ khi tính được tổng điểm thưởng nhận được kể từ trạng thái đó:
\begin{equation*}
G_{t} = \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^{2} \mathit{R}_{t+3} + \dots + \gamma^{T-t-1}\mathit{R}_T
\end{equation*}
trong đó T là thời điểm cuối cùng trong một mẫu thực nghiệm. Ngược với MC, TD(0) thực hiện cập nhật cho một trạng thái dựa trên điểm thưởng vừa nhận được ngay trạng thái đó và giá trị hiện tại của các trạng thái kế tiếp sau đó mà không cần đợi đến khi tính được tổng điểm thưởng:
\begin{equation*}
G_{t}^{(1)} = \mathit{R}_{t+1} + \gamma V_{k}(\mathit{S}_{t+1}) 
\end{equation*}
với $V_k$ là giá trị hiện tại của trạng thái sau khi cập nhật $k$ lần.

Tổng quát, phương pháp $n$-step TD thực hiện cập nhật giá trị ước lượng của một trạng thái $s$ sau $n$ bước kể từ lúc trạng thái $s$ xuất hiện trong mẫu thực nghiệm; và mục tiêu cập nhật được xác định:
\begin{equation}
G_{t}^{(n)} = \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^2 \mathit{R}_{t+3} + \dots + \gamma^{n-1}\mathit{R}_{t+n} + \gamma^{n}V_{k}(\mathit{S}_{t+n}), \forall n \geq 1
\label{eq:n_TD_goal}
\end{equation}
Sau khi xác định mục tiêu cập nhật, việc thực hiện cập nhật giá trị ước lượng của một trạng thái tại lần lặp thứ $k + 1$ tương tự như đã thực hiện ở hai phương pháp trên:
\begin{equation*}
V(\mathit{S}_t) \leftarrow V(\mathit{S}_t) + \alpha \left[ G_{t}^{(n)} - V(\mathit{S}_t) \right]
\end{equation*}

\subsubsection{Phương pháp Sarsa}
Phương pháp Sarsa được dùng để xác định hàm giá trị hành động $q_{\pi}$ thay vì giá trị trạng thái $v_{\pi}$ cho chính sách $\pi$. Trong Sarsa, chúng ta quan tâm chuyển từ cặp trạng thái, hành động này sang cặp trạng thái, hành động khác và học giá trị của những cặp trạng thái, hành động; thay vì chỉ quan tâm đến sự chuyển tiếp trạng thái, cũng như giá trị của chúng như trong TD. Tương tự với TD, Sarsa dựa trên phương pháp "boostrapping" để xác định giá trị điểm thưởng mà hệ thống có thể nhận được ở tương lai từ một thời điểm xác định, đồng thời xác định giá trị của một cặp trạng thái và hành động qua nhiều lần lặp cập nhật.

Phương pháp đơn giản nhất trong Sarsa được gọi là Sarsa(0). Cách thức cập nhật giá trị của một cặp trạng thái và hành động bằng Sarsa(0) được thực hiện:
\begin{align}
Q(\mathit{S}_t, \mathit{A}_t) \leftarrow Q(\mathit{S}_t, \mathit{A}_t) + \alpha \left[\mathit{R}_{t+1} + \gamma Q(\mathit{S}_{t+1}, \mathit{A}_{t+1}) -  Q(\mathit{S}_t, \mathit{A}_t) \right]
\label{eq:Sarsa_action_update}
\end{align}
Giá trị của hành động $\mathit{A}_t$ tại trạng thái $\mathit{S}_t$ được cập nhật dựa trên điểm thưởng từ môi trường ứng với hành động đó và giá trị của hành động ở trạng thái kế tiếp sau đó. Nếu trạng thái tiếp theo $\mathit{S}_{t+1}$ là trạng thái kết thúc khi đó giá trị của các hành động tại trạng thái đó đều có giá trị là không; tức là $Q(\mathit{S}_{t+1}, \mathit{A}_{t+1}) = 0$
Cách thức cập nhật của Sarsa(0) được minh họa trong hình \ref{fig:Sarsa_Zero}.
Cập nhật này sử dụng một bộ gồm 5 phần tử cho cập nhật $(\mathit{S}_t, \mathit{A}_t, \mathit{R}_{t+1}, \mathit{S}_{t+1}, \mathit{A}_{t+1})$.

\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị hành động bằng Sarsa(0)}
	\label{alg_Sarsa_Zero}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $Q$ xấp xỉ hàm giá trị $q_{\pi}$ của chính sách $\pi$
		
		\Operation
		\Repeat
		\State Tạo một mẫu thực nghiệm $E$ bằng chính sách $\pi$
		\State Khởi tạo trạng thái $S$
		\State Chọn một hành động $A$ tại trạng thái $S$ theo chính sách $\pi$
		\For{mỗi bước trong $E$}
		\State Thực hiện hành động $A$; quan sát điểm thưởng $R$ nhận được, và trạng thái tiếp theo $S'$
		\State Chọn hành động $A'$ ở trạng thái $S'$ theo chính sách $\pi$
		\State $Q(S,A) \leftarrow Q(S,A) + \alpha \left[R + \gamma Q(S',A') - Q(S,A)\right]$ \%\% Thực hiện cập nhật giá trị cho cặp trạng thái, hành động $S,A$
		\State $S \leftarrow S'$
		\State $A \leftarrow A'$
		\EndFor
		\Until thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

\begin{figure}
	\centering
	\begin{tikzpicture}[>=stealth,bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node (label1) [align=left]  {Sarsa(0)};
	
	\node [action, label={[name=label node]left:$Q(\mathit{S}_t, \mathit{A}_t)$}] (a1) [below = 0.3cm of label1] {};
	
	\node [state] (s1) [below = 1cm of a1] {}
	edge  node[left]{$\mathit{R}_{t+1}$} (a1);
	
	\node [action, label={[name=label node]below:$Q(\mathit{S}_{t+1}, \mathit{A}_{t+1})$}] (a2) [below = 1cm of s1] {}
	edge  (s1);
	
	\end{scope}	
	\end{tikzpicture}
	\caption[Minh họa phương pháp Sarsa(0)]{Đồ thị minh họa xác định hàm giá trị hành động bằng phương pháp Sarsa(0)}
	\label{fig:Sarsa_Zero}
\end{figure}

Phương pháp $n$-step Sarsa là phương pháp tổng quát cho việc đánh giá chính sách bằng cách xác định hàm giá trị hành động. Mục tiêu cập nhật của $n$-step Sarsa cho một cặp trạng thái và hành động $(\mathit{S}_t, \mathit{A}_t)$ được xác định:
\begin{equation}
G_{t}^{(n)} = \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \dots + \gamma^{n-1} \mathit{R}_{t+n} + \gamma^{n} Q(\mathit{S}_{t+n}, \mathit{A}_{t+1})
\end{equation}
Sau khi xác định được mục tiêu cập nhật, giá trị của một cặp trạng thái và hành đông được cập nhật tương tự như cách cập nhật trong \ref{eq:Sarsa_action_update} của Sarsa(0):
\begin{equation}
Q(\mathit{S}_t, \mathit{A}_t) \leftarrow Q(\mathit{S}_t, \mathit{A}_t) + \alpha \left[G_{t}^{(n)} -  Q(\mathit{S}_t, \mathit{A}_t) \right]
\label{eq:n_Sarsa_update}
\end{equation}

\subsection{Phương pháp cải thiện chính sách}

\subsubsection*{Phương pháp $\epsilon$-greedy}
Trong những bài toán học tăng cường thực tế, chúng ta thường không có mô hình của môi trường như: tập các trạng thái có thể có, xác suất chuyển từ trạng thái này sang trạng thái khác, và điểm thưởng nhận được cho tất cả các trạng thái. Do đó, câu hỏi đặt ra là chúng ta nên thực hiện \textit{khai thác} những kiến thức đã học để tìm lời giải tối ưu hay \textit{khám phá} những kiến thức mới để có cơ hội tìm được lời giải tối ưu hơn cho bài toán. Ví dụ: trong bài toán chọn nhà nhà hàng.
\begin{itemize}
	\item Khai thác: Chọn nhà hàng đã từng vào mà mình thích nhất.
	\item Khám phá: Thử chọn một nhà hàng mới.
\end{itemize}
Nếu chỉ khai thác những kiến thức đã học mà không thực hiện khám phá kiến thức mới thông thường chúng ta chỉ tìm được lời giải tối ưu cục bộ trong những kiến thức đã học. Ngược lại nếu chỉ thực hiện khám phá kiến thức mới mà không thực hiện khai thác kiến thức đã học, chúng ta không thể tìm được lời giải tối ưu.

Ý tưởng của phương pháp $\epsilon$-greedy đảm bảo luôn khám phá kiến thức mới trong quá trình khai thác. Trong bài toán cải thiện chính sách, giả sử chúng có hàm giá trị hành động $Q$, phương pháp $\epsilon$-greedy đảm bảo các hành động luôn có khả năng được chọn thực hiện tại mỗi trạng thái. Phương pháp $\epsilon$-greedy sẽ thực hiện hoàn toàn ngẫu nhiên mà không quan tâm đến hàm giá trị hành động với xác suất $\epsilon$ tại mỗi trạng thái; ngược lại $\epsilon$-greedy sẽ chọn thực hiện hành động dựa trên tham lam hàm giá trị hành động với xác suất $1 - \epsilon$. Qua đó, ta thấy được nếu tại một trạng thái $s$ có thể thực hiện được $m$ hành động khác nhau thì xác xuất để chọn một hành động bất kỳ được chọn mà không quan tâm đến giá trị của chúng là $\epsilon/m$. Từ \ref{eq:epsilon-greedy}, tại một trạng thái $s$, hành động có giá trị lớn nhất sẽ có xác suất được chọn thực hiện là $\epsilon/m + 1 - \epsilon$, ngược lại một hành động không có giá lớn nhất cũng có xác suất được chọn là $\epsilon/m$. 
\begin{equation}
\pi(a|s) =
\left\{\begin{matrix}
\epsilon/m + 1 - \epsilon & \text{nếu }a = \underset{a \in A}{\operatorname{argmax}}Q(s,a)\\ 
\epsilon/m & \text{ngược lại}s
\end{matrix}\right.
\label{eq:epsilon-greedy}
\end{equation}
Câu hỏi đặt một chính sách $\pi'$ thực hiện $\epsilon$-greedy trên hàm giá trị hành động của  chính sách $\pi$ có tốt hơn chính sách $\pi$ không? Theo định lý tối ưu: Với chính sách thực hiện theo phương pháp $\epsilon$-greedy $\pi$ bất kỳ, một chính sách khác $\pi'$ thực hiện $\epsilon$-greedy trên hàm giá trị hành động của chính sách $\pi$, $q_{\pi}$, luôn là một chính sách tốt hơn hoặc bằng chính sách $\pi$, $v_{\pi'}(s) \geq v_{\pi'}(s), \forall s \in \mathcal{S}$.

\subsection{Phương pháp Q-learning}
\paragraph*{On-policy và off-policy}
Ý tưởng của on-policy là dựa trên những kinh nghiệm thực tế của chính hệ thống, nó có thể tự cải thiện trở nên tốt hơn. Những phương pháp thực hiện theo \textit{on-policy} là những phương pháp đánh giá và cải thiện chính sách $\pi$ dựa trên những mẫu dữ liệu có được qua việc tương tác với môi trường theo chính chính sách đó. Các phương pháp quy hoạch động, MC, TD, Sarsa là những phương pháp thực hiện theo on-policy.
Ngược lại với on-policy, ý tưởng của off-policy là hệ thống có thể cải thiện chính sách của nó dựa trên những kinh nghiệm thực tế của một hệ thống có một chính sách thực hiện khác. Những phương pháp thực hiện theo \textit{off-policy} đánh giá và cải thiện chính sách $\pi$ dựa trên những mẫu dữ liệu có được qua việc tương tác với môi trường theo một chính sách $\pi'$ khác. Ngoài ra, những phương pháp thực hiện theo off-policy có thể tận dụng lại những mẫu dữ liệu cũ để tiếp tục cải thiện chính sách.

\paragraph*{Phương pháp off-policy Q-learning}
 Một trong những đột phá quan trọng nhất trong học tăng cường được phát triển dựa trên phương pháp TD được biết đến chính là Q-learning \cite{sutton1998introduction}. Tương tự như những phương pháp ở trên, phương pháp Q-learning xác định giá trị của một cặp trạng thái và hành động bằng cách cập nhật giá trị ước lượng của nó qua nhiều lần lặp:
 \begin{equation}
	 Q(\mathit{S}_t, \mathit{A}_t) \leftarrow Q(\mathit{S}_t, \mathit{A}_t) + \alpha \left[\mathit{R}_{t+1} + \gamma\max_{a}Q(\mathit{S}_{t+1}, a) - Q(\mathit{S}_t, \mathit{A}_t)\right]
 \end{equation}
Mục tiêu cập nhật giá trị của cặp trạng thái, hành động $(\mathit{S}_{t}, \mathit{A}_{t})$ bằng phương pháp Q-learning dựa trên giá trị điểm thưởng nhận được $\mathit{R}_{t+1}$ và giá trị của hành động lớn nhất ở trạng thái kế tiếp $\mathit{S}_{t+1}$. Hình \ref{fig:Q_learning_Alg} minh họa cho cách thức xác định mục tiêu cập nhật của Q-learning. 
Một điểm khác biệt của phương pháp Q-learning so với các phương pháp trên là nó xấp xỉ trực tiếp hàm giá trị hành động tối ưu $q_*$, mà không phải phụ thuộc quá nhiều vào chính sách mà nó đang theo. Ảnh hưởng của chính sách mà hệ thông đang theo đối với phương pháp này là: nó xác định cặp trạng thái và hành động nào được xuất hiện trong các mẫu thực nghiệm. Tuy nhiên để đảm bảo xác định được giá trị hành động tối ưu, một yêu cầu tối thiểu là các cặp trạng thái và hành động đều được xuất hiện trong quá trình đánh giá chính sách và giá trị các cặp trạng thái, hành động đều được cập nhật liên tục. Từng bước thực hiện của phương pháp Q-learning được trình bày trong thuật toán \ref{alg_Q_learning}.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị hành động tối ưu bằng Q-learning}
	\label{alg_Q_learning}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require
		\Ensure Hàm giá trị $Q$ xấp xỉ hàm giá trị $q_{*}$
		
		\Operation
		\Repeat
		\State Tạo một mẫu thực nghiệm $E$ bằng chính sách $\pi'$ (thực hiện $\epsilon$-greedy theo hàm giá trị $Q$ hiện tại)
		\State Khởi tạo trạng thái $S$
		\For{mỗi bước trong $E$}
		\State Chọn một hành động $A$ tại trạng thái $S$ theo chính sách $\pi'$
		\State Thực hiện hành động $A$; quan sát điểm thưởng $R$ nhận được, và trạng thái tiếp theo $S'$
		\State $Q(S,A) \leftarrow Q(S,A) + \alpha \left[R + \gamma\max_{a}Q(S', a) - Q(S,A)\right]$ \%\% Thực hiện cập nhật giá trị cho cặp trạng thái, hành động $S,A$
		\State $S \leftarrow S'$
		\EndFor
		\Until thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	\node [state, label={[name=label node]right:$\mathit{S}_{t}$}] (s0) {};
	
	\node [action, label={[name=label node]right:$\mathit{A}_{t}$}] (a0)  [below = 1 cm of s0] {}
	edge (s0);
	
	\node [state, label={[name=label node]right:$\mathit{S}_{t+1}$}] (s1) [below = 1 cm of a0]     {}
	edge node[right]{$\mathit{R}_{t+1}$} (a0);
	
	\node [action] (a1) [below right = 2 cm and 1 cm of s1, label=right:$a$]    {}
	edge(s1);
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a2--s1--a1};
	
	\node [action] (a3) at ($(a1)!0.5!(a2)$) {}
	edge (s1);	
	\end{scope}
	\end{tikzpicture}
	\caption[Minh họa phương pháp Q-learning]{Đồ thị minh họa cập nhật hàm giá trị hành động bằng phương pháp Q-learning. Q-learning xác định mục tiêu cập nhật cho giá trị của cặp trạng thái, hành động $(\mathit{S}_{t}, \mathit{A}_t)$ bằng tổng giữa điểm thưởng $\mathit{R}_{t+1}$ nhận được khi thực hiện hành động $\mathit{A}_t$ tại trạng thái $\mathit{S}_{t}$ và giá trị hành động $a$ lớn nhất tại trạng thái kế tiếp $\mathit{S}_{t+1}$ đã nhân với hệ số $\gamma$.}
	\label{fig:Q_learning_Alg}
\end{figure}