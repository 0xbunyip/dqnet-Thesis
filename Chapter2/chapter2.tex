\chapter{Kiến thức nền tảng}
\ifpdf
\graphicspath{{Chapter2/Chapter2Figs/PNG/}{Chapter2/Chapter2Figs/PDF/}{Chapter2/Chapter2Figs/}}
\else
\graphicspath{{Chapter2/Chapter2Figs/EPS/}{Chapter2/Chapter2Figs/}}
\fi
\begin{quote}
	\textit{Trong chương này sẽ trình bày những kiến thức nền tảng của học tăng cường. Trong phần đầu tiên chúng em sẽ trình bày định nghĩa của các thành phần cơ bản trong học tăng cường. Tiếp đó sẽ đề cập đến mô hình Markov Decision Processes được áp dụng trong việc đánh giá lý thuyết một số thành phần của bài toán học tăng cường. Cùng với đó sẽ trình bày qui trình tổng quát để đánh giá và cải thiện chính sách trong bài toán. Cuối cùng chúng em sẽ trình bày một số phương pháp phổ biến thường đượcAgent áp dụng để đánh giá cũng như cải thiện giúp hệ thông có cách giải tốt hơn cho bài toán trên.}
\end{quote}

\section{Các thành phần cơ bản của học tăng cường}
\subsection{Agent và môi trường}
Trong học tăng cường, đối tượng học và đưa ra quyết định được gọi chung là \textit{hệ thống}. Nó tương tác trực tiếp tới một đối tượng được gọi là \textit{môi trường}. Sự tương tác này được diễn ra liên tục. Agent lựa chọn hành động dựa trên những gì nó nhận được từ môi trường. Những thông tin này bao gồm:
\begin{itemize}
	\item \textbf{Trạng thái} (state): Những thông tin hữu ích mà hệ thống có thể cảm nhận được từ môi trường. Ví dụ trong đánh cờ, trạng thái có thể là vị trí những quân cờ đang có trên bàn cờ. Thường được ký hiệu là $s$.
	\item \textbf{Điểm thưởng} (reward): Giá trị mà môi trường trả ra tương ứng với trạng thái mà nó vừa đạt được hoặc hành động mà hệ thống vừa thực hiện. Thường được ký hiệu là $r$. Cũng với ví dụ đánh cờ, điểm thưởng mà hệ thống có thể nhận được từ môi trường trong ví dụ này là: +1 nếu hệ thống thắng, -1 nếu hệ thống thua, và trong quá trình đánh cờ điểm thưởng có thể là 0 cho mỗi trạng thái bàn cờ.
\end{itemize}
Từ trạng thái và điểm thưởng nhận được, hệ thống dựa vào đó để ra quyết định chọn hành động phù hợp sao cho cố gắng đạt được nhiều điểm thưởng nhất.

Agent và môi trường tương tác theo một chuỗi tuần tự các time-steps, $t = 0,1,2,...$. Tại mỗi time step $t$, agent nhận những mô tả trạng thái của môi trường, $\mathit{S_t} \in \mathcal{S}$, với $\mathcal{S}$ là tập các trạng thái có thể có. Dựa vào những mô tả trạng thái nhận được, agent chọn một hành động, $\mathit{A_t} \in \mathcal(\mathit{S_t})$, trong đó $\mathcal(\mathit{S_t})$ là tập các hành động có thể thực hiện tại trạng thái $\mathit{S_t}$. Tại time step sau đó, agent nhận được giá trị điểm thưởng, $\mathit{R_{t+1}} \in \mathbb{R}$, cùng với trạng thái tiếp theo $\mathit{S_{t+1}}$ Quá trình tương tác giữa agent và môi trường được mô tả trong hình \ref{AgentEnvironment}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{AgentEnvironment}
	\caption{Quá trình tương tác giữa hệ thông và môi trường}
	\label{AgentEnvironment}
\end{figure}

Các thành phần của agent gồm có:
\begin{itemize}
	\item \textbf{Chính sách}. Chính sách, $\pi$, xác định khả năng chọn một hành động khi hệ thống nhận được một trạng thái $s$. Chính xác tại time step $t$ được xác định $\pi_t(a \mid s) = \mathbb{P}[\mathit{A_t} = a \mid \mathit{S_t} = s]$. Để đạt được mục tiêu được nhiều điểm thưởng nhất, hệ thống cần có một chính sách chọn lựa hành động phù hợp mỗi khi gặp một trạng thái. Những phương pháp học tăng cường thường tập trung thay đổi các chính sách của hệ thống sao cho đạt được kết quả tốt trong thực nghiệm.
	\item \textbf{Hàm giá trị}. Hầu hết các thuật toán học tăng cường đầu tập trung đánh giá những hàm giá trị, các hàm này đánh giá một trạng thái hoặc hành động là tốt như thế nào cho agent thông qua việc ước lượng điểm thưởng mà hệ thống có thể nhận được ở tương lai. Thông thường, giá trị của một trạng thái $s$, dưới một chính sách $\pi$  được ký hiệu $v_{\pi}(s)$ là lượng điểm thưởng kỳ vọng nhận được bắt đầu từ trạng thái $s$ về sau.
	\item \textbf{Mô hình}. Trong một số bài toán học tăng cường, hệ thống có thể xây dựng mô hình cho riêng mình để mô phỏng lại môi trường. Qua đó cho phép hệ thống có thể suy luận hoặc dự đoán những thông tin mà nó có thể nhận được từ môi trường trong tương lai.
\end{itemize}	

\subsection{Returns}
Return $\mathit{G_t}$ xác định lượng điểm thưởng mà agent nhận được kể từ thời điểm time step $t$ đến tương lai. Return thường được xác định bằng nhiều hàm khác nhau, trong đó hàm đơn giản nhất xác định return bằng tổng các điểm thưởng có thể nhận được. Nó có dạng như sau:
\begin{equation}
\mathit{G_t} = \mathit{R_{t+1}} + \mathit{R_{t+2}} + ... + \mathit{R_{T}}
\end{equation}	
ở đây $T$ là time step cuối cùng.

Mặt khác, return cũng có thể được xác định bằng tổng điểm thưởng đã bị discount qua từng time step. Nó được định nghĩa như sau:
\begin{equation}
\mathit{G_t} = \mathit{R_{t+1}} + \gamma\mathit{R_{t+2}} + \gamma^{2}\mathit{R_{t+3}}... + \gamma^{T-1}\mathit{R_{T}} = \sum_{k=0}^{\infty}\gamma^{k}\mathit{R_{t+k+1}}
\end{equation}
Trong đó $\gamma$ là một hệ số với giá trị $0\leqslant \gamma \leqslant 1$. $\gamma$ cũng được gọi là tỉ lệ discount. Tỉ lệ này xác định độ tin tưởng của agent vào giá trị điểm thưởng ở tương lai. Khi $\gamma \to 1$, agent có su hướng quan tâm đến giá trị điểm thưởng tương lai càng nhiều. Đặc biệt với $\gamma = 0$, khi đó agent chỉ quan tâm giá trị điểm thưởng ở hiện tại mà bỏ qua những giá trị điểm thưởng ở tương lai.	

\section{Mô hình Markov Decision Processes}
%	\begin{itemize}
%			\item Các thành phần MDP
%			\item Ví dụ cho mô hình MDP
%			\item Phương trình Bellman
%			\item Qui trình đánh giá chính sách: Kỹ thuật qui hoạch động
%			\item Qui trình cải thiện chính động: Kỹ thuật qui hoạch động
%	\end{itemize}
\subsection{Định nghĩa mô hình Markov Decision Processes}
Mô hình Markov Decision Processes (MDP) được sử dụng để mô hình hóa bài toán học tăng cường một cách có hình thức. Cụ thể, MDP là một bộ bao gồm 5 thành phần $<\mathcal{S, A, P, R, \gamma}>$ trong đó:
\begin{itemize}
	\item $\mathcal{S}$: tập trạng thái hữu hạn có thể có của môi trường.
	\item $\mathcal{A}$: tập những hành động hữu hạn mà hệ thống có thể thực hiện để tương tác với môi trường.
	\item $\gamma$: Hệ số có giá trị thỏa $0\leqslant \gamma \leqslant 1$ thể hiện mức độ tin tưởng về giá trị điểm thưởng nhận được ở tương lai.
	\item $\mathcal{P}$: ma trận xác suất chuyển trạng thái. Trong đó $\mathcal{P}_{ss'}^{a}$ là xác suất chuyển đến trạng thái $s'$ khi hệ thống đang ở trạng thái $s$ và thực hiện hành động $a$.
	\begin{equation}
	\mathcal{P}_{ss'}^{a} = \mathbb{P}[\mathit{S_{t+1}} = s' \mid \mathit{S_{t}} = s, \mathit{A_{t}} = a]
	\end{equation}
	\item $\mathcal{R}$: ma trận điểm thưởng theo từng bộ (trạng thái, hành động). $\mathcal{R}_{s}^a$ là kỳ vọng giá trị điểm thưởng nhận được khi hệ thống thực hiện hành động $a$ ở trạng thái $s$.
	\begin{equation}
	\mathcal{R}_{s}^a = \mathbb{E}[\mathit{R_{t}} \mid \mathit{S_{t}} = s, \mathit{A_{t}} = a]
	\end{equation}				
\end{itemize}

\textbf{Ví dụ: Mô hình MDP trong robot thu gom} Công việc của robot này là thu lượm những lon soda đã được uống hết trong văn phòng. Nó có những cảm biến để xác định những lon soda này, bánh xe và cánh tay để di chuyển và gắp nhặt những lon này bỏ vào thùng. Robot hoạt động bằng pin sạc. Hệ thống điều khiển của robot có chức năng tiếp nhận những thông tin từ cảm biến từ đó điểu khiển bánh xe và cánh tay. Trong ví dụ, chúng em chỉ xét dựa trên mức độ pin hiện tại robot nên quyết định tìm kiếm những lon soda như thế nào? Robot có thể có ba quyết định (1) thực hiện tìm kiếm một lon soda, (2) đứng yên và đợi người khác mang lon soda đến cho nó, (3) quay trở lại nơi sạc pin. Trạng thái của môi trường được xác định là trạng thái của pin hiện tại của robot. Cách tốt nhất để tìm kiếm những lon soda là robot thực hiện hành động tìm kiếm, nhưng việc này sẽ làm giảm dung lượng của pin. Ngược lại nếu robot đứng yên và đợi thì dung lượng pin của nó không giảm. Mỗi khi dung lượng pin của robot ở mức thấp nó sẽ quay lại chỗ sạc pin. Trường hợp xấu nhất có thể xảy ra là robot không đủ dung lượng pin để quay lại nơi sạc khi đó nó sẽ đứng yên và đợi ai đó mang nó đến chỗ sạc. Do đó robot cần có một chiến lược phù hợp để đạt được hiệu năng cao nhất có thể.
Hệ thống đưa ra những quyết định của nó dựa trên mức năng lượng pin. Mức năng lượng này có thể được xác định hai mức \textit{cao} và \textit{thấp}. Khi đó tập trạng thái mà hệ thống có thể nhận được $\mathcal{S} = \left \{\text{cao}, \text{thấp} \right \}$. Những hành động của hệ thống trong ví dụ này được xét đơn giản gồm ba hành động \textit{đợi}, \textit{tìm kiếm}, và \textit{sạc pin}. Khi dung lượng pin ở trạng thái cao, hệ thống chỉ thực hiện hai hành động: tìm kiếm và đợi. Ngược lại khi ở trạng thái thấp, hệ thống có thể thực hiện ba hành động: tìm kiếm, đợi, và sạc pin.
$$\mathcal{A}(\text{cao}) =  \left \{\text{tìm kiếm}, \text{đợi} \right \}$$
$$\mathcal{A}(\text{thấp}) =  \left \{\text{tìm kiếm}, \text{đợi}, \text{sạc pin} \right \}$$

Khi mức năng lượng pin ở mức cao, việc robot thực hiện tìm kiếm sẽ có xác suất $\alpha$ năng lượng pin vẫn ở mức cao, và $1 - \alpha$ năng lượng của pi sẽ chuyển về mức thấp. Mặt khác, khi mức năng lượng ở mức thấp, nếu robot thực hiện tìm kiếm sẽ có xác suất $\beta$ năng lượng pin ở mức thấp và $1 - \beta$ chuyển đến mức cao, trường hợp này xảy ra khi dung lượng pin cạn kiệt và cần ai đó mang nó đến chỗ sạc cho đến khi đạt mức năng lượng cao. Ngoài ra, mỗi lần robot thu gom được một lon soda nó sẽ nhận được $+1$ điểm thưởng và sẽ bị $-3$ điểm thưởng mỗi khi nó phải cần ai đó mang đến chỗ sạc. $r_{\text{đợi}}$, $r_{\text{tìm kiếm}}$ là số lượng lon soda kỳ vọng mà robot có thể thu gom được trong khi đợi và tìm kiếm. Hình \ref{graphRobot} minh họa cho mô hình MDP trong robot thu gom.

%% Hình vẽ
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=6.5cm,>=stealth',bend angle=45 ,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=blue!20,minimum size = 15mm]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state] (high)                           {Cao};	
	\node [state] (low) [right of=high]              {Thấp};
	
	\node [action] (w1) [above = 1.5cm of high, label=left:Đợi] {}
	edge      					(high)
	edge [post,bend right] node[left]{$1, r_{\text{đợi}}$}      (high);
	
	\node [action] (s1) [below = 1.5cm of high, label=right:Tìm kiếm] {}
	edge        				(high)
	edge [post,bend left] node[left]{$\alpha, r_{\text{tìm kiếm}}$}       (high)
	edge [post,bend right] node[below=0.2]{$1 - \alpha, r_{\text{tìm kiếm}}$}       (low);
	
	\node [action] (s2) [above = 1.5cm of low, label=left:Tìm kiếm] {}
	edge        				(low)
	edge [post,bend left] node[right]{$\beta, r_{\text{tìm kiếm}}$}       (low)
	edge [post,bend right] node[above=0.2]{$1 - \beta, -3$}       (high);
	
	\node [action] (w2) [below = 1.5cm of low, label=right:Đợi] {}
	edge        				(low)
	edge [post,bend right] node[right]{$1, r_{\text{đợi}}$}       (low);
	
	\node [action] (re) [left = 1.5cm of low, , label=above:Sạc pin] {}
	edge						(low)
	edge [post] node[above]{$1, 0$} (high);
	\end{scope}		
	\end{tikzpicture}
	\caption[Đồ thị minh họa chuyển trạng thái cho robot thu gom]{Đồ thị minh họa chuyển trạng thái cho robot thu gom. Trong đồ thị có hai loại node: node trạng thái và node hành động. Node trạng thái minh họa những trạng thái có thể có mà hệ thống có thể nhận được, nó được ký hiệu một vòng tròn lớn với tên của trạng thái bên trong. Node hành động tường ứng với cặp (trạng thái, hành động). Việc thực hiện hành động $a$ tại trạng thái $s$ tương ứng trên đồ thị là một cạnh bắt đầu từ node trạng $s$ tới node hành động $a$. Khi đó môi trường sẽ trả ra trạng thái tiếp theo $s'$ ứng với đích của mũi tên đi từ node hành động $a$. Xác suất chuyển tới trạng thái $s'$ khi thực hiện hành động $a$ ở trạng thái $s$ $p(s' \mid s,a)$, và giá trị điểm thưởng kỳ vọng nhận được trong trường hợp này $r(s,a,s')$ tương ứng với ký hiệu trên mũi tên. Ví dụ: khi mức năng lượng pin đăng ở trạng thái \textit{thấp}, hệ thống quyết định thực hiện hành động \textit{sạc pin} khi đó trạng thái tiếp theo mà hệ thống nhận được sẽ là mức năng lượng pin ở trạng thái \textit{cao} và xác suất chuyển tới trạng thái \textit{cao} $p(\text{cao} \mid \text{thấp}, \text{sạc pin})$ là 1 và giá trị kỳ vọng điểm thưởng tương ứng $r(\text{thấp}, \text{sạc pin}, \text{cao})$ là 0.}
	\label{graphRobot}
\end{figure}

%% ----------------------------------------------------

\subsection{Chính sách và hàm giá trị} \label{sec:policy_value}
Một chính sách $\pi$ xác định xác suất mà hệ thông thực hiện hành động $a$ khi nó trong trạng thái $s$ được ký hiệu $\pi(a \mid s)$. Có thể nói chính sách như "bộ não" của hệ thống, nó quyết định cách thức mà hệ thống hành động trong những trạng thái cụ thể do đó một chính sách tốt cũng làm cho khả năng hệ thống ra quyết định trở nên tốt hơn.

Hàm giá trị cho biết những trạng thái hoặc những cặp hành động và trạng thái tốt như thế nào cho hệ thông khi nó trong những trạng thái hoặc thực hiện những cặp hành động và trạng thái đó. Khái niệm tốt ở đây nghĩa là giá trị điểm thưởng kỳ vọng mà hệ thông có thể nhận được ở tương lai. Hầu hết các thuật toán trong học tăng cường đều tập trung vào việc đánh giá những hàm giá trị qua đó cải thiện chính sách trở nên tốt hơn. Điểm thưởng mà hệ thống có thể nhận được trong tương lai phụ thuộc vào những hành động mà nó thực hiện. Do đó hàm giá trị chịu ảnh hưởng rất nhiều vào chính sách. Giá trị của trạng thái $s$ dưới một chính sách $\pi$, ký hiệu $v_{\pi}(s)$, là giá trị kỳ vọng của return mà hệ thống nhận được bắt đầu từ trạng thái $s$ theo chính sách $\pi$ sau đó. Với mô hình MDP, $v_{\pi}(s)$ được định nghĩa như sau:
\begin{equation}
v_{\pi}(s) = \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s\right ] = \mathbb{E}_{\pi}\left [\sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+1} \middle|\ \mathit{S}_t= s\right ]
\end{equation}
$v_{\pi}$ được gọi là hàm giá trị trạng thái dưới chính sách $\pi$.

Tương tự, chúng ta định nghĩa giá trị của việc thực hiện hành động $a$ trong trạng thái $s$ dưới chính sách $\pi$, được ký hiệu $q_{\pi}(s,a)$, là giá trị kỳ vọng của return mà hệ thống nhận được bắt đầu từ việc thực hiện hành động $a$ trong trạng thái $s$ theo chính sách $\pi$
\begin{equation}
\label{action_value}
q_{\pi}(s,a) = \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s, \mathit{A}_{t} = a  \right ] = \mathbb{E}_{\pi}\left [\sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+1} \middle|\ \mathit{S}_t= s, \mathit{A}_{t} = a \right ]
\end{equation}
$q_{\pi}$ được gọi là hàm giá trị hành động dưới chính sách $\pi$.

Hình \ref{fig:relationship_value_functions} minh họa quan hệ giữa hàm giá trị trạng thái và hàm giá trị hành động, khi có được hàm giá trị này ta có thể có được hàm giá trị còn lại. Phương trình \ref{eq:relation_value_action} xác hàm giá trị của một trạng thái bằng giá trị kỳ vọng giá trị của các hành động thực hiện tại trạng thái đó. Hình \ref{fig:relationship_value_functions}a minh họa quan hệ giữa giá trị của một trạng thái $s$ và giá trị của các hành động thực hiện tại trạng thái đó. Hình \ref{fig:relationship_value_functions}b cho thấy từ việc thực hiện hành động $a$ tại trạng thái $s$, môi trường có thể trả ra nhiều trạng thái tiếp theo $s'$ khác nhau. Do đó giá trị của hành động $a$ ở trạng thái $s$ có thể được xác định bằng tổng giá trị kỳ vọng điểm thưởng nhận được và giá trị kỳ vọng của các trạng thái tiếp theo đó đã được nhân với hệ số $\gamma$. Cách xác định này được biểu diễn trong phương trình \ref{eq:relation_action_value}.
\begin{align}
v_{\pi} = {} & \sum_{a \in \mathcal{A}(s)}^{}\pi(a \mid s)q_{\pi}(s,a) \label{eq:relation_value_action}\\
q_{\pi}(s,a) = {} & \mathcal{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{}\mathcal{P}_{ss'}^{a}v_{\pi}(s') \label{eq:relation_action_value}
\end{align}
%----------------------------------------	
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state, label={[name=label node]right:$s \mapsto v_{\pi}(s)$}] (s1)      {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action, label={[name=label node]right:$a \mapsto q_{\pi}(s,a)$}] (a1) [below right = 2 cm and 1 cm of s1]    {}
	edge node[right]{$r$} (s1);	
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1);
	
	\end{scope}		
	
	\begin{scope}[xshift=8cm]
	% Second net		
	\node [action, label={[name=label node]right:$s,a \mapsto q_{\pi}(s,a)$}] (a4)      {};		
	\node (label) [left = 2cm of a4] {(b)};		
	\node [state] (s8) [below right = 2 cm and 1 cm of a4, label=right:$s' \mapsto v_{\pi}(s')$]    {}
	edge node[right]{$r$} (a4);		
	\node [state] (s9) [below left  = 2 cm and 1 cm of a4]    {}
	edge (a4);		
	\end{scope}		
	\end{tikzpicture}		
	\caption[Đồ thị minh họa quan hệ giữa những hàm giá trị]{Đồ thị minh họa quan hệ giữa hàm giá trị trạng thái và hàm giá trị hành động}
	\label{fig:relationship_value_functions}
\end{figure}

%----------------------------------------
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\label{fig_bellman_value}
	\node [state, label=above:$s$] (s1)                           {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action] (a1) [below = 2cm of s1]              {}
	edge (s1);
	
	\node [action] (a2) [left = 2cm of a1]              {}
	edge (s1);
	
	\node [action] (a3) [right = 2cm of a1, label=right:$a$]              {}
	edge (s1);
	
	\node [state] (s2) [below left = 2cm and 0.3cm of a1] {}
	edge (a1);
	
	\node [state] (s3) [below right = 2cm and 0.3cm of a1] {}
	edge (a1);
	
	\node [state] (s4) [below left = 2cm and 0.3cm of a2] {}
	edge (a2);
	
	\node [state] (s5) [below right = 2cm and 0.3cm of a2] {}
	edge (a2);
	
	\node [state] (s6) [below left = 2cm and 0.3cm of a3] {}
	edge (a3);
	
	\node [state] (s7) [below right = 2cm and 0.3cm of a3, label=right:$s'$] {}
	edge node[right]{$r$} (a3);	
	\end{scope}
	
	\begin{scope}[xshift=8cm]
	% Second net
	\label{fig_bellman_action}
	\node [action, label={[name=label node]above:$s,a$}] (a4)      {};
	
	\node (label) [left = 2cm of a4] {(b)};
	
	\node [state] (s8) [below right = 2 cm and 1 cm of a4, label=right:$s'$]    {}
	edge node[right]{$r$} (a4);	
	
	\node [state] (s9) [below left  = 2 cm and 1 cm of a4]    {}
	edge (a4);
	
	\node [action] (a5) [below left  = 2 cm and 0.3 cm of s8]    {}
	edge (s8);
	
	\node [action] (a6) [below right  = 2 cm and 0.3 cm of s8, label=right:$a'$]    {}
	edge (s8);
	
	\node [action] (a7) [below left  = 2 cm and 0.3 cm of s9]    {}
	edge (s9);
	
	\node [action] (a8) [below right  = 2 cm and 0.3 cm of s9]    {}
	edge (s9);	
	
	\end{scope}
	
	\end{tikzpicture}
	\caption[Đồ thị minh họa cho hàm giá trị]{Đồ thị minh họa cho (a) $v_{\pi}$ và (b) $q_{\pi}$}
	\label{backup_diagram}
\end{figure}

Hàm giá trị có một tính chất cơ bản thường được áp dụng trong học tăng cường đó là mối quan hệ đệ quy. Cho bất kỳ chính sách $\pi$ với bất kỳ trạng thái $s$, hàm giá trị cho một trạng thái được xác định:	
\begin{align}
\label{bell_state}
%\begin{aligned}
v_{\pi}(s) = {}& \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s\right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^{2} \mathit{R}_{t+3} + ... \mid \mathit{S}_t = s  \right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma( {R}_{t+2} + \gamma \mathit{R}_{t+3} + ...) \mid \mathit{S}_t = s  \right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma\mathit{G}_{t + 1} \mid \mathit{S}_t = s  \right ] \nonumber \\
= {}& \mathbb{E}_{\pi}\left [ \mathit{R}_{t+1} + \gamma v(\mathit{S}_{t+1}) \mid \mathit{S}_t = s  \right ]
%\end{aligned}
\end{align}	
Phương trình \ref{bell_state} được gọi là phương trình Bellman cho $v_{\pi}$. Từ phương trình này ta thấy được mối liên quan giữa giá trị của một trạng $s$ bất kỳ và giá trị của những trạng thái tiếp theo đạt được từ trạng thái đó. Ý tưởng nhìn trước một bước, hay nói cách khác đánh giá trạng thái hiện tại bằng cách nhìn trước tất cả những trạng tái tiếp theo có thể đạt được từ trạng thái đó, được minh họa trong hình \ref{backup_diagram}a. Từ một trạng thái, môi trường có thể trả ra nhiều điểm thưởng $r$ và trạng thái tiếp theo $s'$ khác nhau. Phương trình \ref{bell_state} sẽ trung bình tất cả các trường hợp có thể đó lại theo xác suất mà chúng xuất hiện. Phương trình này cũng cho thấy giá trị của một trạng thái phải bằng tổng giá trị kỳ vọng của những trạng thái tiếp sau đó và giá trị kỳ vọng điểm thưởng nhận được.

\begin{equation}
\label{bell_action}
q_{\pi}(s,a) = \mathbb{E}_{\pi} \left[\mathit{R}_{t+1} + \gamma q_{\pi}(\mathit{S}_{t+1}, \mathit{A}_{t+1}) \mid \mathit{S}_t = s, \mathit{A}_t = a \right]
\end{equation}

Phân tích phương trình \ref{action_value} tương tự như đã làm đối với hàm giá trị hành động, ta có được phương trình \ref{bell_action}. Hình \ref{backup_diagram}b minh họa ý tưởng nhìn trước một đước để đánh giá giá trị của một hành động ở trạng thái hiện tại. Từ một hành động $a$ ở trạng thái $s$, môi trường có thể trả ra nhiều điểm thưởng $r$ và trạng thái $s'$ khác nhau. Trong mỗi trạng thái $s'$ lại có nhiều hành động $a'$ khác nhau có thể thực hiện.	Phương trình \ref{bell_action} sẽ trung bình tất cả các trường hợp có thể đó lại theo xác suất mà chúng được thực hiện. Hay nói cách khác, phương trình \ref{bell_action} cho thấy giá trị của một hành động $a$ tại trạng thái $s$, $q_{\pi}(a,s)$ cũng được xác định tổng bằng giá trị kỳ vọng điểm thưởng hệ thống nhận được nhận được ngay sau khi thực hiện thực hiện hành động đó và giá trị kỳ vọng của các hành động trong những trạng thái kế tiếp.

\subsection{Hàm giá trị tối ưu}
Để giải quyết những vấn đề trong học tăng cường, chúng ta cần tìm một chính sách sao cho hệ thống có thể đạt được nhiều điểm thưởng nhất có thể. Một chính sách $\pi$ được xác định là tốt hơn hoặc bằng chính sách $\pi'$ khi giá trị kỳ vọng của return theo chính sách $\pi$ lớn hơn hoặc bằng giá trị đó theo chính sách $\pi'$. Hay có thể định nghĩa theo cách khác:
\begin{equation}
\pi \geq \pi' \Longleftrightarrow v_{\pi}(s) \geq v_{\pi'}(s), \forall s \in \mathcal{S}
\end{equation}

Luôn có ít nhất một chính sách tốt hơn hoặc bằng tất cả các chính sách còn lại \cite{sutton1998introduction}. Chúng được gọi chung là \textit{chính sách tối ưu} và được ký hiệu $\pi_{*}$. Những chính sách tối ưu đều cùng có chung một hàm giá trị trạng thái và hàm giá trị hành động. Hai loại hàm giá trị này có thể được gọi chung là \textit{hàm giá trị tối ưu}. Chúng ta cũng có thể gọi tách biệt \textit{hàm giá trị trạng thái tối ưu} đối với hàm giá trị trạng thái và \textit{hàm giá trị hành động tối ưu} đối với hàm giá trị hành động. Phương trình \ref{eq:optimal_state} và \ref{eq:optimal_action} định nghĩa hình thức cho hai loại hàm này
\begin{align}
v_{*}(s) = {} & \max_{\pi}v_{\pi}(s), \forall s \in \mathcal{S} \label{eq:optimal_state} \\
q_{*}(s,a) = {} & \max_{\pi}q_{\pi}(s,a), \forall s \in \mathcal{S} \text{ và } \forall a \in \mathcal{A}(s) \label{eq:optimal_action}
\end{align}
Từ hai phương trình \ref{eq:optimal_state} và \ref{eq:optimal_action} thấy rằng để xác định hàm giá trị tối ưu của mỗi trạng thái $s$ hoặc cặp trạng thái và hành động ($s,a$), ta cần thử đánh giá giá trị của chúng theo tất cả các chính sách có thể có và chọn giá trị cao nhất là giá trị tối ưu cho trạng thái $s$ hoặc cặp trạng thái và hành động ($s,a$).

Hình \ref{fig:relationship_optimal_functions} minh họa quan hệ giữa giá trị trạng thái tối ưu và hàm giá trị hành động tối ưu, khi có được hàm này ta dễ dàng có được hàm còn lại. Trong hình \ref{fig:relationship_optimal_functions}a, ta có thể xác định giá trị tối ưu cho trạng thái $s$ dựa trên hàm giá trị hành động tối ưu của các hành động có thể thực hiện tại trạng thái đó. Phương trình \ref{eq:optimal_state_action} xác định giá trị tối ưu cho trạng thái $s$ bằng cách chọn giá trị hành động tối ưu lớn nhất trong các hành động có thể thực hiện ở trọng thái đó. Tương tự trong hình \ref{fig:relationship_optimal_functions}b, ta có thể xác định giá trị tối ưu cho hành động $a$ ở trạng thái $s$, dựa trên hàm giá trị trạng thái tối ưu của các trạng thái kế tiếp đạt được từ hành động đó. Phương trình \ref{eq:optimal_action_state} xác định giá trị tối ưu của hành động $a$ tại trạng thái $s$ bằng tổng giá trị kỳ vọng điểm thưởng nhận được từ môi trường và giá trị tối ưu kỳ vọng của những trạng thái kế tiếp đã nhân với hệ số $\gamma$.

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state, label={[name=label node]right:$s \mapsto v_{*}(s)$}] (s1)      {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action, label={[name=label node]right:$a \mapsto q_{*}(s,a)$}] (a1) [below right = 2 cm and 1 cm of s1]    {}
	edge node[right]{$r$} (s1);	
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a2--s1--a1};
	
	\end{scope}		
	
	\begin{scope}[xshift=8cm]
	% Second net		
	\node [action, label={[name=label node]right:$s,a \mapsto q_{*}(s,a)$}] (a4)      {};		
	\node (label) [left = 2cm of a4] {(b)};		
	\node [state] (s8) [below right = 2 cm and 1 cm of a4, label=right:$s' \mapsto v_{*}(s')$]    {}
	edge node[right]{$r$} (a4);		
	\node [state] (s9) [below left  = 2 cm and 1 cm of a4]    {}
	edge (a4);		
	\end{scope}		
	\end{tikzpicture}		
	\caption[Đồ thị minh họa quan hệ giữa những hàm giá trị tối ưu]{Đồ thị minh họa quan hệ giữa hàm giá trị trạng thái tối ưu và hàm giá trị hành động tối ưu}
	\label{fig:relationship_optimal_functions}
\end{figure}	
\begin{align}
%		q_{*}(s,a) = \mathbb{E} \left[ \mathit{R}_{t+1} + \gamma v_{*}(\mathit{S}_{t+1}) \mid \mathit{S}_{t} = s, \mathit{A}_{t} = a \right]
v_{*}(s) = {} & \max_{a \in \mathcal{A}(s)} q_{*}(s,a) \label{eq:optimal_state_action}\\
q_{*}(s,a) = {} & \mathit{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{} \mathcal{P}_{ss'}^{a} v_{*}(s') \label{eq:optimal_action_state} \\
v_{*}(s) = {} & \max_{a \in \mathcal{A}(s)} \mathit{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{} \mathcal{P}_{ss'}^{a} v_{*}(s') \label{eq:bellman_optimal_state}\\
q_{*}(s,a) = {} & \mathit{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{} \mathcal{P}_{ss'}^{a} \max_{a' \in \mathcal{A}(s')} q_{*}(s',a') \label{eq:bellman_optimal_action}		
\end{align}

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	
	\node [state, label={[name=label node]right:$s \mapsto v_{*}(s)$}] (s1)      {};
	
	\node (label) [left = 2cm of s1] {(a)};
	
	\node [action] (a1) [below right = 2 cm and 1 cm of s1, label=right:$a$]    {}
	edge(s1);
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a2--s1--a1};
	
	\node [state] (s2) [below left  = 2 cm and 0.3 cm of a1]    {}
	edge (a1);
	
	\node [state, label={[name=label node]right:$s' \mapsto v_{*}(s')$}] (s3) [below right  = 2 cm and 0.3 cm of a1]    {}
	edge node[right]{$r$}	 (a1);
	
	\node [state] (s4) [below left  = 2 cm and 0.3 cm of a2]    {}
	edge (a2);
	
	\node [state] (s5) [below right  = 2 cm and 0.3 cm of a2]    {}
	edge (a2);	
	\end{scope}
	
	\begin{scope}[xshift=8cm]
	
	\node [action, label={[name=label node]right:$(s,a) \mapsto q_{*}(s,a)$}] (a3)      {};
	
	\node (label) [left = 2cm of a3] {(b)};
	
	\node [state] (s6) [below right = 2 cm and 1 cm of a3, label=right:$s'$]    {}
	edge node[right]{$r$} (a3);	
	
	\node [state] (s7) [below left  = 2 cm and 1 cm of a3]    {}
	edge (a3);
	
	\node [action] (a4) [below left  = 2 cm and 0.3 cm of s6]    {}
	edge (s6);
	
	\node [action, label={[name=label node]right:$a' \mapsto q_{*}(s',a')$}] (a5) [below right  = 2 cm and 0.3 cm of s6]    {}
	edge (s6)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a4--s6--a5};
	
	\node [action] (a6) [below left  = 2 cm and 0.3 cm of s7]    {}
	edge (s7);
	
	\node [action] (a7) [below right  = 2 cm and 0.3 cm of s7]    {}
	edge (s7)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a6--s7--a7};
	
	\end{scope}		
	\end{tikzpicture}
	\caption[Đồ thị minh họa phương trình Bellman trong hàm giá trị tối ưu]{Đồ thị minh họa phương trình Bellman trong (a) $v_{*}$} và (b) $q_{*}$
	\label{fig:bellman_optimal_function}
\end{figure}

Phương trình \ref{eq:bellman_optimal_state} và \ref{eq:bellman_optimal_action} dễ dàng có được bằng cách thay thế hai phương trình \ref{eq:optimal_state_action} và \ref{eq:optimal_action_state} qua lại lẫn nhau. Từ hai phương trình này, ta thấy được dạng phương trình Bellman trong hàm giá trị trạng thái tối ưu và hàm giá trị hành động tối ưu. Hình \ref{fig:bellman_optimal_function} minh họa ý tưởng nhìn trước một bước của phương trình Bellman trong hàm giá trị tối ưu. Trong đó hình \ref{fig:bellman_optimal_function}a minh họa cách thức xác định giá trị tối ưu cho một trạng thái ứng với phương trình \ref{eq:bellman_optimal_state}. Hình \ref{fig:bellman_optimal_function}b minh họa cách thức xác định giá trị tối ưu của một hành động ở một trạng thái ứng với phương trình \ref{eq:bellman_optimal_action}.

\section{Quy trình lặp chính sách}

%	\begin{itemize}
%		\item Dẫn nhập: Trên thực tế ta không có thông tin về môi trường
%		\item Qui trình đánh giá chính sách
%			\begin{itemize}
%				\item[+] Dựa trên hàm giá trị trạng thái: Monte-Carlo, TD(0), n-step TD, TD($\lambda$)
%				\item[+] Dựa trên hàm giá trị hành động: Monte-Carlo, Sarsa(0), n-step Sarsa, Sarsa($\lambda$)
%			\end{itemize}
%		\item Qui trình cải thiện chính sách: Phương pháp greedy
%	\end{itemize}

%	Chúng ta vừa định nghĩa hàm giá trị và chính sách tối ưu. Một hệ thống khi có được chính sách tối ưu thì làm việc rất tốt. Nhưng trong thực thế điều này hiếm khi xảy ra, thông thường để có được hàm giá trị hoặc chính sách tối ưu yêu cầu chi phí tính toán là rất lớn. Thậm chí, dù ta biết được mô hình hoàn toàn của môi trường, chúng ta vẫn không thể đạt được chính sách tối ưu bằng cách giải phương trình Bellman của hàm giá trị tối ưu. Do đó ta chỉ có thể xấp xỉ chính sách hoặc hàm giá trí tối ưu bằng những phương pháp xấp xỉ với các mức độ khác nhau. Ví dụ cụ thể cho vấn đề này đó là cờ vây, mặc dù số trạng thái và hành động có thể đều có thể tính toán được, nhưng chi phí tính toán để tìm một chính sách đánh cờ tối ưu là rất lớn. Do đó người chơi chỉ có thể dựa trên kinh nghiệm có để đi nước cờ tốt nhất cho mình.	Một vấn đề khác mà chúng ta phải đối mặt đó là khả năng lưu trữ có hạn. Thông thường để lưu tất cả các trạng thái hoặc các cặp trạng thái và hành động trong các bài toán học tăng cường  yêu cầu bộ nhớ rất lớn. Mặt khác, trong thực tế, chúng ta thường không biết đầy đủ về môi trường như những trạng thái có thể có, điểm thưởng của một trạng thái bất kỳ, và ma trận chuyển trạng thái. Do đó, ta cần những phương pháp có thể học từ những
Trong các bài toán học tăng cường, mục tiêu chính của ta là tìm được chính sách tối ưu $\pi_{*}$ nhằm giúp cho hệ thông giải quyết bài toán tốt nhất có thể. Do đó ta cần có quy trình để thay đổi chính sách hiện tại trở nên tối ưu. Quy trình này được gọi là \textit{quy trình lặp chính sách}. Hình \ref{fig:policy_iteration} minh họa quy trình chung của lặp chính sách. Trong quy trình này được chia thành hai giai đoạn:
\begin{itemize}
	\item \textbf{Đánh giá chính sách}: Việc đánh giá một chính sách $\pi$ được thực hiện bằng cách xác định hàm giá trị trạng thái của dưới chính sách đó.
	\item \textbf{Cải thiện chính sách}: Sau khi có được hàm giá trị của một chính sách $\pi$, chính sách cải thiện mới $\pi'$ được tạo ra bằng cách thực hiện tham lam trên hàm giá trị của chính sách $\pi$, tức là chỉ chọn thực hiện hành động có giá trị cao nhất dựa trên hàm giá trị trạng thái; việc này có thể thực hiện được do mối quan hệ giữa hai loại hàm. 
\end{itemize}
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=5.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=blue!20,minimum size = 15mm]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node (policy)                           {\large $\pi$};	
	
	\node (value) [right of=policy]              {\large $\mathcal{V}$}
	edge [post,bend left] node[below]{Cải thiện} node[above=0.1cm]{$\pi \to \text{greedy}(V)$}       (policy)
	edge [pre,bend right] node[above]{Đánh giá}  node[below=0.1cm]{$V \to V^{\pi}$}     (policy);	
	
	\end{scope}			
	\end{tikzpicture}
	\caption[Quy trình lặp chính sách]{Quy trình chung trong lặp chính sách}
	\label{fig:policy_iteration}
\end{figure}
Một chính sách $\pi_{1}$ được cải thiện từ chính sách $\pi_{0}$ dựa trên hàm giá trị trạng thái $v_{\pi_{0}}$. Khi có được chính sách $\pi_{1}$ ta có thể tính được hàm giá trị $v_{\pi_{1}}$ qua đó tiếp tục cải thiện để có được chính sách $\pi_{2}$. Quá trình này diễn ra cho đến khi đạt được chính sách tối ưu.
$$\pi_{0} \xrightarrow{\text{Đánh giá}} v_{\pi_{0}} \xrightarrow{\text{Cải thiện}} \pi_{1} \xrightarrow{\text{Đánh giá}} v_{\pi_{1}} \xrightarrow{\text{Cải thiện}} \pi_{2} \cdots \xrightarrow{\text{Cải thiện}} \pi_{*} \xrightarrow{\text{Đánh giá}} v_{\pi_{*}}$$

\subsection{Phương pháp đánh giá chính sách}
Trong phần này, chúng em sẽ trình bày một số phương pháp được áp dụng để đánh giá chính sách.
\subsubsection{Phương pháp quy hoạch động (Dynamic Programming)}
Quy hoạch động thường được dùng để giải quyết các bài toán tối ưu mà dữ liệu có tính thứ tự, ví dụ như dữ liệu chuỗi hay dữ liệu thời gian. Một bài toán tối ưu có thể được giải quyết bằng quy hoạch động cần có hai đặc điểm:
\begin{itemize}
	\item Quy tặc tối ưu (Principle of Optimality): các bài toán có thể phân rã thành các bài toán con, và kết quả của bài toán con này đóng góp vào lời giải của bài toán gốc.
	\item Các bài toán con chồng lấn lên nhau và lặp lại nhiều lần: nhằm tận dụng lại kết quả của những bài toán con đã tính toán trước đó.
\end{itemize}

Trong nhiều bài toán học tăng cường, kỹ thuật quy hoạch động được dùng để tìm chính sách tối ưu hoặc tối ưu hàm giá trị. Để có thể áp dụng kỹ thuật quy hoạch động, những bài toán này cũng cần phải thỏa yêu cầu là hệ thống có kiến thức đầy đủ về môi trường hay cách khác môi trường có mô hình MDP.
Quy hoạch động xác định hàm giá trị của một chính sách bằng cách cập nhật hàm giá trị được khởi tạo bất kỳ ban đầu qua nhiều vòng lặp, dựa vào phương trình Bellman. Ý tưởng của cách xác định này như sau: Ban đầu khởi tạo hàm giá trị $v_0$ bất kỳ cho tất cả các trạng thái, trừ trạng thái kết thúc được luôn có giá trị là 0. Tiến hành cập nhật hàm giá trị mới $v_1$ cho chính sách dựa trên hàm giá trị $v_0$ theo phương trình \ref{eq:DP_Value_Update}. Tương tự cập nhật hàm giá trị mới $v_2$ dựa trên $v_1$. Quá trình lặp cho đến khi độ khác biệt giữa hàm giá trị sau và giá trị trước đó nhỏ hơn một lượng cho trước. Quy trình cập nhật được minh họa trong hình \ref{fig:Update_Value_DP}, trong đó giá trị mới $v_{k+1}$ của trạng thái $s$ được xác định dựa trên giá trị kỳ vọng điểm thưởng nhận được theo chính sách $\pi$, và giá trị hiện tại $v_{k}$ của các trạng thái $s'$ kế tiếp trạng thái $s'$. Tổng thể của việc đánh giá chính sách bằng quy hoạch động được trình bày ở thuật toán \ref{alg_DP}
\begin{equation}
v_{k+1}(s) \leftarrow \sum_{a \in \mathcal{A}}^{}\pi(a \mid s)(\mathcal{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{}\mathcal{P}_{ss'}^{a}v_{k}(s'))
\label{eq:DP_Value_Update}
\end{equation}		
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	
	\node [state, label={[name=label node]right:$s \mapsto v_{k+1}(s)$}] (s1)      {};
	
	\node [action] (a1) [below right = 2 cm and 2 cm of s1, label=right:$a$]    {}
	edge(s1);
	
	\node [action] (a2) [below left  = 2 cm and 2 cm of s1]    {}
	edge (s1);
	
	\node [state] (s2) [below left  = 2 cm and 1 cm of a1]    {}
	edge (a1);
	
	\node [state, label={[name=label node]right:$s' \mapsto v_{k}(s')$}] (s3) [below right  = 2 cm and 1 cm of a1]    {}
	edge node[right]{$r$}	 (a1);
	
	\node [state] (s4) [below left  = 2 cm and 1 cm of a2]    {}
	edge (a2);
	
	\node [state] (s5) [below right  = 2 cm and 1 cm of a2]    {}
	edge (a2);	
	\end{scope}
	
	\end{tikzpicture}
	\caption[Cập nhật hàm giá trị bằng quy hoạch động]{Đồ thị minh họa cập nhật hàm giá trị bằng quy hoạch động}
	\label{fig:Update_Value_DP}
\end{figure}

\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị bằng quy hoạch động}
	\label{alg_DP}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\State Khởi tạo ngẫu nhiên $V(s)$ cho tất cả trạng thái $s$ không phải trạng thái kết thúc. Nếu $s$ là trạng thái kết thúc, $V(s) = 0$
		\Repeat
		\State $\Delta \leftarrow 0$ \%\% Tính độ khác biệt giữa hàm giá trị cũ và giá trị mới. Độ lớn của $\Delta$ được xác định là độ khác biệt lớn nhất giữa giá trị cũ và giá trị mới của một trạng thái trong tất cả các trạng thái.
		\For{$s \in \mathcal{S}$} \%\% Với mỗi trạng thái
		\State $v \leftarrow  V(s)$ \%\% Lưu giá trị hiện tại của trạng thái $s$
		\State $V(s) \leftarrow \sum_{a \in \mathcal{A}}^{}\pi(a \mid s)(\mathcal{R}_{s}^{a} + \gamma \sum_{s' \in \mathcal{S}}^{}\mathcal{P}_{ss'}^{a}V(s'))$ \%\% Tính giá trị mới cho trạng thái $s$ dựa trên giá trị hiện tại của các trạng thái $s'$ kế tiếp của trạng thái $s$, và giá trị kỳ vọng của các hành động tại trạng thái đó theo chính sách $\pi$.
		\State $\Delta \leftarrow \max(\Delta, \left |v - V(s) \right |)$ \%\% Cập nhật giá trị mới cho $\Delta$
		\EndFor
		\Until $\Delta < \theta$ (Một lượng đủ nhỏ) 
	\end{algorithmic}
\end{algorithm}

[TODO] Ví dụ minh họa TD

Mặc dù đã quy hoạch động đã được chứng minh là xấp xỉ tốt hay thậm chí là tìm được hàm giá trị trạng thái của chính sách $\pi$ \cite{gordon1995stable}, nhưng trong các bài toán học thực tế của học tăng cường đặc biệt là những bài toán lớn thì quy hoạch động trở nên không khả thi do chi phí tính toán cao, trong trường hơp xấu nhất chi phí tính toán thuộc $O(k^{n})$ với $k$ là số hành động và $n$ là số trạng thái. Ngoài ra, trong nhiều bài toán thực tế thông thường chúng ta không có kiến thức đầy đủ về môi trường như ma trận chuyển trạng thái $\mathcal{P}$, ma trận điểm thưởng $\mathcal{R}$, tập các trạng thái $\mathcal{A}$. Do đó hệ thống phải có khả năng học từ những thông tin mà nó tiếp nhận được qua việc tương tác với môi trường. Các thông tin này thường ở dạng chuỗi (trạng thái, hành động, điểm thưởng)
$\mathit{S}_1, \mathit{A}_1, \mathit{R}_2, \mathit{S}_2, \mathit{A}_2, \mathit{R}_3, \dots, \mathit{S}_T$. Với những đặc điểm đó, quy hoạch động không thể áp dụng để đánh giá chính sách trong các bài toán này.

\subsubsection{Phương pháp Monte Carlo (MC)}
Tương tự với quy hoạch động, Monte Carlo (MC) xác định hàm giá trị của một chính sách bằng cách cập nhật hàm giá trị khởi tạo qua nhiều vòng lặp. Điểm biệt khác với quy hoạch động của phương pháp MC là nó có thể áp dụng để đánh giá chính sách khi hệ thông không có kiến thức đầy đủ về môi trường. MC dựa trên những thông tin mà hệ thông có được qua việc tương tác với môi trường để xấp xỉ hàm giá trị. Thông thường những thông tin này được chia thành các \textit{episode}. Mỗi episode là một chuỗi bắt đầu từ một trạng thái bất kỳ cho đến khi đạt được một trong những trạng thái kết thúc. Khi đó MC chỉ thực hiện cập nhật hàm giá trị khi kết thúc một episode.

Ý tưởng của MC là xác định giá trị của một trạng thái $s$ qua các mẫu thực nghiệm. Phương pháp MC xác định giá trị của trạng thái $s$ bằng cách trung bình những return mà hệ thông nhận được sau khi quan sát được trạng thái $s$. Khi quan sát càng nhiều mẫu thực nghiệm có trạng thái $s$ xuất hiện, giá trị trung bình sẽ càng xấp xỉ tốt giá trị thực của trạng thái này theo chính sách $\pi$.

Một mẫu thực nghiệm là những thông tin có được trong quá trình hệ thống tương tác với môi trường bằng chính sách $\pi$. Giá trị của trạng thái $s$, $v(s)$ được tính dựa trên những mẫu thực nghiệm có trạng thái $s$ xuất hiện. Một trạng thái $s$ có thể xuất hiện nhiều lần trong một mẫu thực nghiệm. Lần xuất hiện đầu tiên của trạng thái $s$ trong một mẫu thực nghiệm được gọi là first-visit trạng thái đó. Phương pháp first-visit MC xác định giá trị trạng thái $s$ $v_{\pi}(s)$ bằng trung bình tất cả return mà hệ thống nhận sau lần first-visit của trạng thái $s$ trong các mẫu thực nghiệm. Tổng thể của việc đánh giá chính sách bằng first-visit MC được trình bày ở thuật toán \ref{alg_MC}. Hình \ref{fig:first_visit_MC} minh họa cách thức cập nhật hàm giá trị trên một episode.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị trạng thái bằng phương pháp first-visit MC}
	\label{alg_MC}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\State Khởi tạo ngẫu nhiên $V(s)$ cho tất cả trạng thái $s$ không phải trạng thái kết thúc. Nếu $s$ là trạng thái kết thúc, $V(s) = 0$
		\State Khởi tạo danh sách rỗng \textbf{Returns($s$)} cho tất cả trạng thái $s \in \mathcal{S}$ \%\% Danh sách Returns($s$) chứa tất cả các return mà hệ thống nhận được sau lần first-visit của trạng thái $s$ trong các episode.
		\Repeat
		\State Tạo một episode $E$ bằng chính sách $\pi$
		\For{mỗi trạng thái $s$ xuất hiện lần đầu trong $E$}
		\State $G \leftarrow$ return nhận được sau lần xuất hiện đầu tiên của $s$
		\State Thêm $G$ vào danh sách Returns(s)
		\State $V(s) \leftarrow$ average(Returns(s))
		\EndFor
		\Until Thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}
\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 10mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	\tikzstyle{terminal}=[square,draw=black!60,
	fill=black!75,minimum size=10mm]
	
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node [state, label={[name=label node]below:$\mathit{S}_0$}] (s1)                           {};
	
	\node [action] (a1) [right = 0.7cm of s1]{}
	edge (s1);
	
	\node[state, fill=green, label={[name=label node]below:$\mathit{S}_1$}] (s2)  [right = 0.7cm of a1]{}
	edge node[above]{$r_1$} (a1);
	
	\node [action] (a2) [right = 0.7cm of s2]{}
	edge (s2);
	
	\node[state, fill=red, label={[name=label node]below:$\mathit{S}_2$}] (s3)  [right = 0.7cm of a2]{}
	edge node[above]{$r_2$} (a2);
	
	\node [action] (a3) [right = 0.7cm of s3]{}
	edge (s3);
	
	\node (etc) [right = 0.7cm of a3] {\large $\dots$};
	
	\node [state, fill=green, label={[name=label node]below:$\mathit{S}_{T-1}$}] (s4) [right = 0.7cm of etc]{};
	
	\node [action] (a4) [right = 0.7cm of s4]{}
	edge (s4);
	
	\node [state, fill = gray, label={[name=label node]below:Trạng thái kết thúc}] (t) [right = 0.7cm of a4]{}
	edge node[above]{$r_{T}$} (a4)
	edge [post,bend right] (s3)
	edge [post,bend right] (s2)
	edge [post,bend right] node[above]{Cập nhật giá trị} (s1);
	
	\end{scope}	
	\end{tikzpicture}
	\caption[Cập nhật hàm giá trị bằng phương pháp Monte Carlo]{Đồ thị minh họa cập nhật hàm giá trị trên một episode bằng phương pháp first-visit MC. Hình tròn lớn được ký cho trạng thái xuất hiện. Hình tròn nhỏ được ký cho hành động thực hiện. Màu xác khác nhau giữa các hình tròn biểu thị cho sự khác nhau giữa các trạng thái. Phương pháp first-visit MC chỉ cập nhật giá trị cho các trạng thái khi kết thúc một episode, và mỗi trạng thái chỉ được cập nhật một lần mặc dù trạng thái đó có thể xuất hiện nhiều lần trong một episode}
	\label{fig:first_visit_MC}
\end{figure}

Trong nhiều trường hợp, hệ thống không có được mô hình của môi trường, việc sử dụng hàm giá trị hành động trở nên khả thi hơn hàm giá trị trạng thái. Với việc có được mô hình của môi trường, hàm giá trị trạng thái là đủ để cải thiện một chính xách trở nên tốt hơn; nó đơn giản là nhìn trước trạng thái tiếp theo và chọn bất kỳ hành động nào dẫn đến trạng thái đó mà đạt được nhiều điểm thưởng nhất. Ngược lại, nếu không có được mô hình của môi trường, hàm giá trị trạng thái là không đủ do hệ thống không thể xác định được trạng thái tiếp theo là trạng thái gì. Vì vậy, nó cần đánh giá giá trị của mỗi hành động trong mỗi trạng thái để xác định hành động nào nên thực hiện ở mỗi trạng thái qua đó cải thiện chính sách đang thực hiện.
Việc xác định hàm giá trị hành đọng $q_{\pi}$ được thực hiện tương tự như đã làm với hàm giá trị trạng thái $v_{\pi}$. Để xác định giá trị của hành động $a$ tại trạng thái $s$, nó thực hiện tính trung bình các return mà hệ thộng nhận được dựa vào các mẫu thực nghiệm có sự xuất hiện của cặp trạng thái hành động $(s,a)$. Lẫn xuất hiện đầu tiên của cặp trạng thái và hành động $(s,a)$ trong một mẫu thực nghiệm được gọi là first-visit của cặp trạng thái và hành động đó. Phương pháp first-visit MC xác định giá trị của hành động $a$ ở trạng thái $s$, $q_{\pi}(s,a)$ bằng trung bình tất cả các return nhận được sau lần first-visit của cặp $(s,a)$ trong các mẫu thực nghiệm. Thuật toán \ref{alg_MC_action} trình bày cách thức xác định hàm giá trị hành động bằng first-visit MC.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị hành động bằng phương pháp first-visit MC}
	\label{alg_MC_action}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\State Khởi tạo ngẫu nhiên $Q(s,a)$ cho tất cả các cặp trạng thái, hành động $s,a$.
		\State Khởi tạo danh sách rỗng \textbf{Returns($s,a$)} cho tất cả các cặp trạng thái, hành động ($s,a$). \%\% Danh sách Returns($s,a$) chứa tất cả các return mà hệ thống nhận được sau lần first-visit của cặp trạng thái, hành động ($s,a$) trong các episode.
		\Repeat
		\State Tạo một episode $E$ bằng chính sách $\pi$
		\For{mỗi cặp trạng thái, hành động $(s,a)$ xuất hiện lần đầu trong $E$}
		\State $G \leftarrow$ return nhận được sau lần xuất hiện đầu tiên của cặp trạng thái, hành động $(s,a)$
		\State Thêm $G$ vào danh sách Returns($s,a$)
		\State $Q(s,a) \leftarrow$ average(Returns($s$))
		\EndFor
		\Until Thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

\subsubsection{Phương pháp Temporal Difference (TD)}
Phương pháp Temporal Difference (TD) kết hợp ý tưởng giữa Monte Carlo và quy hoạch động. Giống như Monte Carlo, phương pháp TD có thể học trực tiếp từ các mẫu thực nghiệm có được qua việc tương tác của hệ thống với môi trường mà không cần có mô hình của môi trường. Mặt khác tương tự với quy hoạch động, phương pháp TD thựa hiện cập nhật giá trị dựa trên những phần đã được xác định trước đó mà không phải đợi đến khi kết thúc một episode như MC.

Giả sử ta có các trung bình $\mu_{1}$, $\mu_{2}$, ... của chuỗi $x_1, x_2, ...$ có thể được tính như sau:
\begin{align}
\mu_{k} = {}& \frac{1}{k}\sum_{j=1}^{k}x_{j} \notag \\
= {}& \frac{1}{k} \left(x_k + \sum_{j = 1}^{k - 1}x_j\right) \notag \\
= {}& \frac{1}{k}(x_k + \left(k - 1\right)\mu_{k-1}) \notag \\
= {}& \mu_{k - 1} + \frac{1}{k}(x_k - \mu_{k-1}) \label{eq:mean}
\end{align}
Phương pháp Monte Carlo phải đợi cho đến khi tính được return sau lần xuất hiện của một trạng thái để thực hiện cập nhật giá trị cho trạng thái đó, và giá trị của một trạng thái được cập nhật qua nhiều vòng lặp. Dựa vào phương trình \ref{eq:mean} giá trị của mỗi trạng thái có thể được cập nhật như sau:
\begin{align}
N(\mathit{S}_{t}) & \leftarrow N(\mathit{S}_{t}) + 1 \\
V(\mathit{S}_t) & \leftarrow V(\mathit{S}_t) + \frac{1}{N(\mathit{S}_{t})}(\mathit{G_t} - V(\mathit{S}_t)) \label{eq:update_value_MC}
\end{align}
Khi đó $\mathit{G}_t$ được gọi là mục tiêu cập nhật cho $V(S_t)$. Mặt khác, khi môi trường không ổn định việc cập nhật giá trị trạng thái theo \ref{eq:update_value_MC}  thường được cố định bằng hệ số $\alpha$:
\begin{equation}
V(\mathit{S}_t) \leftarrow V(\mathit{S}_t) + \alpha(\mathit{G_t} - V(\mathit{S}_t))
\end{equation}
Khác với phương pháp MC, phương pháp TD chỉ cần đợi tới bước tiếp theo ngay sau đó $t+1$ để hình thành một cái đích cho việc cập nhật qua việc quan sát điểm thưởng $R_{t+1}$ và giá trị của trạng thái tiếp theo $V(\mathit{S}_{t+1})$. Phương pháp TD đơn giản nhất được gọi là TD(0). Cách thức cập nhật giá trị của một trạng thái trong phương pháp này như sau:
\begin{equation}
V(\mathit{S}_t) \leftarrow 	V(\mathit{S}_t) + \alpha[\mathit{R}_{t+1} + \gamma V(\mathit{S}_{t+1}) - V(\mathit{S}_t)]
\label{eq:update_value_TD}
\end{equation}
Trong \ref{eq:update_value_TD} ta thấy mục tiêu cập nhật cho $V(\mathit{S}_t)$ trong TD(0) là $\mathit{R}_{t+1} + \gamma V(\mathit{S}_{t+1})$. Vì phương pháp TD thực hiện cập nhật giá trị của một trạng thái dựa một phần vào các giá trị của những trạng thái tiếp theo nên phương pháp này là một phương pháp "bootstapping", tương tự với quy hoạch động. Như đã định nghĩa trong \ref{sec:policy_value}, giá trị của trạng thái $s$ dưới chính sách $\pi$ được xác định:
\begin{align}
v_{\pi}(s) = {} & \mathbb{E}_{\pi}\left [\mathit{G}_t \mid \mathit{S}_{t} = s\right ] \label{eq:value_base_return} \\
={} & \mathbb{E}_{\pi}\left [\sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+1} \middle|\ \mathit{S}_t= s\right ] \notag \\
= {} & \mathbb{E}_{\pi}\left [\mathit{R}_{t+1} + \gamma \sum_{k = 0}^{\infty}\gamma^{k}\mathit{R}_{t+k+2} \middle|\ \mathit{S}_t= s\right ] \notag \\
= {} & \mathbb{E}_{\pi} \left[\mathit{R}_{t+1} + \gamma v_{\pi}(\mathit{S}_{t+1}) \mid \mathit{S}_t = s\right] \label{eq:value_base_bootstrap}
\end{align}
Qua đó, ta thấy rằng phương pháp MC sử dụng ước lượng của \ref{eq:value_base_return} là mục tiêu cập nhật; trong khi đó phương pháp quy hoạch động sử dụng ước lượng của \ref{eq:value_base_bootstrap}. Mục tiêu cập nhật của MC là một ước lượng vì không biết giá trị kỳ vọng trong \ref{eq:value_base_return} do đó một mẫu return được sử dụng để thay thế cho giá trị kỳ vọng thực sự của nó. Mục tiêu cập nhật của quy hoạch động cũng là một ước lượng không phải vì giá trị kỳ vọng, do trong quy hoạch động chúng ta giả định hệ thống có mô hình của môi trường, nhưng là vì $v_{\pi}(\mathit{S}_{t+1})$ là không biết do hiện tại nó đang được đánh giá; do đó $V(\mathit{S}_{t+1})$ được sử dụng để thay thế. Mục tiêu cập nhất trong TD là một ước lượng do cả hai nguyên nhân trên nên TD ước lượng giá trị kỳ vọng trong \ref{eq:value_base_return} qua mẫu và sử dụng ước lượng của hàm giá trị hiện tại $V$ để thay thế cho hàm giá trị đúng $v_{\pi}$. Vì vậy, phương pháp TD được cho là phương pháp kết hợp cách lấy mẫu của MC và bootstrapping của quy hoạch động. Từng bước thực hiện cập nhật hàm giá trị bằng phương pháp TD(0) được trình bày trong thuật toán thuật toán \ref{alg_TD_Zero}.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị trạng thái bằng TD(0)}
	\label{alg_TD_Zero}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $V$ xấp xỉ hàm giá trị $v_{\pi}$ của chính sách $\pi$
		
		\Operation
		\Repeat
		\State Tạo một mẫu thực nghiệm $E$ bằng chính sách $\pi$
		\State Khởi tạo trạng thái $s$
		\For{mỗi bước trong $E$}
		\State $A \leftarrow$ hành động được chọn theo chính $\pi$ tại $s$
		\State Thực hiện hành động $A$; quan sát điểm thưởng $r$ nhận được, và trạng thái tiếp theo $s'$
		\State $V(s) \leftarrow V(s) + \alpha \left[r + \gamma V(s') - V(s)\right]$ \%\% Thực hiện cập nhật giá trị cho trạng thái $s$
		\State $s \leftarrow s'$
		\EndFor
		\Until thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

Với bất kỳ chính sách $\pi$ cố định. Việc xác định hàm giá trị bằng phương pháp TD đã được chứng minh hội tu về hàm giá trị $v_{\pi}$ theo luật số lớn. Trong thực nghiệm, phương pháp TD thường hội tụ về hàm giá trị $v_{\pi}$ nhanh hơn phương pháp MC.

Phương pháp TD(0) xem giá trị của các trạng thái kế tiếp từ một trạng thái $s$ là giá trị đại diện cho điểm thưởng mà trạng thái $s$ có thể nhận được ở tương lai; và dựa vào giá trị đại diện này và điểm thưởng nhận được ngay trạng thái $s$ để xác định giá trị của trạng thái đó. Tổng quát cho phương pháp TD là n-step TD. Để đánh xác định giá trị của một trạng thái $s$, phương pháp n-step TD xem giá trị của trạng thái thứ $n$ sau đó là giá trị đại diện cho điểm thưởng mà hệ thống có thể nhận được từ bước thứ $n$ trở về sau; và xác định giá trị của trạng thái $s$ dựa trên giá trị đại diện này cùng với điểm thưởng đã nhận được ở $n$ bước sáu đó. Hình \ref{fig:n_TD} minh họa xác định hàm giá trị trạng thái bằng phương pháp $n$-step TD. Phương pháp này thực hiện cập nhật cho một trạng thái ở bước thứ $n$ sau khi trạng thái $s$ xuất hiện trong mẫu thực nghiệm. 
\begin{figure}
	\centering
	\begin{tikzpicture}[>=stealth,bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node (label1) [align=left]  {TD($1$-step)};
	
	\node [state, label=left:$V(\mathit{S}_t)$] (s1) [below = 0.3cm of label1] {};
	
	\node [action] (a1) [below = 0.5cm of s1] {}
	edge  (s1);
	
	\node [state, label={[name=label node]below:$V(\mathit{S}_{t+1})$}] (s2) [below = 0.5 of a1] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a1);
	
	%--------------------------------
	\node (label2) [right = 1cm of label1] {2-step};
	
	\node [state] (s3) [below = 0.3cm of label2] {};
	
	\node [action] (a2) [below = 0.5cm of s3] {}
	edge (s3);
	
	\node [state] (s4) [below = 0.5 of a2] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a2);
	
	\node [action] (a3) [below = 0.5cm of s4] {}
	edge (s4);
	
	\node [state] (s5) [below = 0.5 of a3, label=below:$V(\mathit{S}_{t+2})$] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a3);
	
	%--------------------------------
	\node (label3) [right = 1cm of label2] {3-step};
	
	\node [state] (s6) [below = 0.3cm of label3] {};
	
	\node [action] (a4) [below = 0.5cm of s6] {}
	edge (s6);
	
	\node [state] (s7) [below = 0.5 of a4] {}
	edge  node[left]{$\mathit{R}_{t+1}$} (a4);
	
	\node [action] (a5) [below = 0.5cm of s7] {}
	edge (s7);
	
	\node [state] (s8) [below = 0.5 of a5] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a5);
	
	\node [action] (a6) [below = 0.5cm of s8] {}
	edge (s8);
	
	\node [state, label=below:$V(\mathit{S}_{t+3})$] (s9) [below = 0.5 of a6] {}
	edge node[left]{$\mathit{R}_{t+3}$} (a6);
	
	%--------------------------------
	\node (labeln) [right = 2cm of label3] {$n$-step};
	
	\node [state] (s10) [below = 0.3cm of labeln] {};
	
	\node [action] (a7) [below = 0.5cm of s10] {}
	edge (s10);
	
	\node [state] (s11) [below = 0.5 of a7] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a7);
	
	\node (dot2) [left = 1 of s11] {\Huge \dots};
	
	\node [action] (a8) [below = 0.5cm of s11] {}
	edge (s11);
	
	\node [state] (s12) [below = 0.5 of a8] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a8);	
	
	\node [action] (a9) [below = 0.5cm of s12] {}
	edge (s12);
	
	\node (dot1) [below = 0.5 of a9] {\huge \vdots};
	
	\node [action] (a10) [below = 0.5cm of dot1] {};
	
	\node [state, label=below:$V(\mathit{S}_{t+n})$] (s12) [below = 0.5 of a10] {}
	edge node[left]{$\mathit{R}_{t+n}$} (a10);
	
	%--------------------------------
	\node (labelMC) [right = 2cm of labeln] {Monte Carlo};
	
	\node [state] (s13) [below = 0.3cm of labelMC] {};
	
	\node [action] (a11) [below = 0.5cm of s13] {}
	edge (s13);
	
	\node [state] (s14) [below = 0.5 of a11] {}
	edge node[left]{$\mathit{R}_{t+1}$} (a11);
	
	\node (dot3) [left = 1 of s14] {\Huge \dots};
	
	\node [action] (a12) [below = 0.5cm of s14] {}
	edge (s14);
	
	\node [state] (s15) [below = 0.5 of a12] {}
	edge node[left]{$\mathit{R}_{t+2}$} (a12);
	
	
	\node [action] (a13) [below = 0.5cm of s15] {}
	edge (s15);
	
	\node [state] (s16) [below = 0.5 of a13] {}
	edge node[left]{$\mathit{R}_{t+3}$} (a13);
	
	\node [action] (a14) [below = 0.5cm of s16] {}
	edge (s16);
	
	\node [state] (s17) [below = 0.5 of a14] {}
	edge node[left]{$\mathit{R}_{t+4}$} (a14);
	
	\node [action] (a15) [below = 0.5cm of s17] {}
	edge (s17);
	
	\node [state] (s18) [below = 0.5 of a15] {}
	edge node[left]{$\mathit{R}_{t+5}$} (a15);
	
	\node [action] (a16) [below = 0.5cm of s18] {}
	edge (s18);
	
	\node (dot4) [below = 0.5cm of a16] {\huge \vdots};
	
	\node [action] (a17) [below = 0.5 of dot4] {};
	
	\node [state, fill = gray, label={[name=label node]below:Trạng thái kết thúc}] (s19) [below = 0.5 of a17] {}
	edge node[left]{$\mathit{R}_{T}$} (a17);
	
	\end{scope}	
	\end{tikzpicture}
	\caption[Minh họa phương pháp $n$-step TD]{Đồ thị bên trái ngoài cùng minh họa xác định hàm giá trị bằng phương pháp TD(0), trong khi đó đồ thị bên phải ngoài cùng minh họa cho phương pháp Monte Carlo. Các đồ thị ở giữa minh họa phương pháp $n$-step TD ứng với từng giá trị của $n$}
	\label{fig:n_TD}
\end{figure}

Xét một chuỗi trạng thái, điểm thưởng $\mathit{S}_{t}, \mathit{R}_{t+1}, \mathit{S}_{t+1}, \mathit{R}_{t+2}, \dots, \mathit{S}_{T}$. Như chúng ta đã biết, Monte Carlo thực hiện cập nhật ước lượng giá trị của trạng thái $s$ chỉ khi tính được return của trạng thái đó:
\begin{equation*}
	G_{t} = \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^{2} \mathit{R}_{t+3} + \dots + \gamma^{T-t-1}\mathit{R}_T
\end{equation*}
trong đó T là thời điểm cuối cùng trong một mẫu thực nghiệm. Ngược với MC, TD(0) thực hiện cập nhật cho một trạng thái dựa trên điểm thưởng vừa nhận được ngay trạng thái đó và giá trị hiện tại của các trạng thái kế tiếp sau đó mà không cần đợi đến khi tính được return:
\begin{equation*}
G_{t}^{(1)} = \mathit{R}_{t+1} + \gamma V_{k}(\mathit{S}_{t+1}) 
\end{equation*}
với $V_k$ là giá trị hiện tại của trạng thái sau khi cập nhật $k$ lần.

Tổng quát, phương pháp $n$-step TD thực hiện cập nhật giá trị ước lượng của một trạng thái $s$ sau $n$ bước kể từ lúc trạng thái $s$ xuất hiện trong mẫu thực nghiệm; và mục tiêu cập nhật được xác định:
\begin{equation}
G_{t}^{(n)} = \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^2 \mathit{R}_{t+3} + \dots + \gamma^{n-1}\mathit{R}_{t+n} + \gamma^{n}V_{k}(\mathit{S}_{t+n}), \forall n \geq 1
\label{eq:n_TD_goal}
\end{equation}
Sau khi xác định mục tiêu cập nhật, việc thực hiện cập nhật giá trị ước lượng của một trạng thái tại lần lặp thứ $k + 1$ tương tự như đã thực hiện ở hai phương pháp trên:
\begin{equation*}
V(\mathit{S}_t) \leftarrow V(\mathit{S}_t) + \alpha \left[ G_{t}^{(n)} - V(\mathit{S}_t) \right]
\end{equation*}

\subsubsection{Phương pháp Sarsa}
Phương pháp Sarsa được dùng để xác định hàm giá trị hành động $q_{\pi}$ thay vì giá trị trạng thái $v_{\pi}$ cho chính sách $\pi$. Trong Sarsa, chúng ta quan tâm chuyển từ cặp trạng thái, hành động này sang cặp trạng thái, hành động khác và học giá trị của những cặp trạng thái, hành động; thay vì chỉ quan tâm đến sự chuyển tiếp trạng thái, cũng như giá trị của chúng như trong TD. Tương tự với TD, Sarsa dựa trên phương pháp "boostrapping" để xác định giá trị điểm thưởng mà hệ thống có thể nhận được ở tương lai từ một thời điểm xác định, đồng thời xác định giá trị của một cặp trạng thái và hành động qua nhiều lần lặp cập nhật.

Phương pháp đơn giản nhất trong Sarsa được gọi là Sarsa(0). Cách thức cập nhật giá trị của một cặp trạng thái và hành động bằng Sarsa(0) được thực hiện:
\begin{align}
Q(\mathit{S}_t, \mathit{A}_t) \leftarrow Q(\mathit{S}_t, \mathit{A}_t) + \alpha \left[\mathit{R}_{t+1} + \gamma Q_(\mathit{S}_{t+1}, \mathit{A}_{t+1}) -  Q(\mathit{S}_t, \mathit{A}_t) \right]
\label{eq:Sarsa_action_update}
\end{align}
Giá trị của hành động $\mathit{A}_t$ tại trạng thái $\mathit{S}_t$ được cập nhật dựa trên điểm thưởng từ môi trường ứng với hành động đó và giá trị của hành động ở trạng thái kế tiếp sau đó. Nếu trạng thái tiếp theo $\mathit{S}_{t+1}$ là trạng thái kết thúc khi đó giá trị của các hành động tại trạng thái đó đều có giá trị là không; tức là $Q(\mathit{S}_{t+1}, \mathit{A}_{t+1}) = 0$
Cách thức cập nhật của Sarsa(0) được minh họa trong hình \ref{fig:Sarsa_Zero}.
Cập nhật này sử dụng một bộ gồm 5 phần tử cho cập nhật $(\mathit{S}_t, \mathit{A}_t, \mathit{R}_{t+1}, \mathit{S}_{t+1}, \mathit{A}_{t+1})$.

\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị hành động bằng Sarsa(0)}
	\label{alg_Sarsa_Zero}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require Chính sách $\pi$ cần đánh giá
		\Ensure Hàm giá trị $Q$ xấp xỉ hàm giá trị $q_{\pi}$ của chính sách $\pi$
		
		\Operation
		\Repeat
		\State Tạo một mẫu thực nghiệm $E$ bằng chính sách $\pi$
		\State Khởi tạo trạng thái $S$
		\State Chọn một hành động $A$ tại trạng thái $S$ theo chính sách $\pi$
		\For{mỗi bước trong $E$}
		\State Thực hiện hành động $A$; quan sát điểm thưởng $R$ nhận được, và trạng thái tiếp theo $S'$
		\State Chọn hành động $A'$ ở trạng thái $S'$ theo chính sách $\pi$
		\State $Q(S,A) \leftarrow Q(S,A) + \alpha \left[R + \gamma Q(S',A') - Q(S,A)\right]$ \%\% Thực hiện cập nhật giá trị cho cặp trạng thái, hành động $S,A$
		\State $S \leftarrow S'$
		\State $A \leftarrow A'$
		\EndFor
		\Until thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

\begin{figure}
	\centering
	\begin{tikzpicture}[>=stealth,bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	% First net
	\node (label1) [align=left]  {Sarsa(0)};
	
	\node [action, label={[name=label node]left:$Q(\mathit{S}_t, \mathit{A}_t)$}] (a1) [below = 0.3cm of label1] {};
	
	\node [state] (s1) [below = 1cm of a1] {}
	edge  node[left]{$\mathit{R}_{t+1}$} (a1);
	
	\node [action, label={[name=label node]below:$Q(\mathit{S}_{t+1}, \mathit{A}_{t+1})$}] (a2) [below = 1cm of s1] {}
	edge  (s1);
	
	\end{scope}	
	\end{tikzpicture}
	\caption[Minh họa phương pháp Sarsa(0)]{Đồ thị minh họa xác định hàm giá trị hành động bằng phương pháp Sarsa(0)}
	\label{fig:Sarsa_Zero}
\end{figure}

Phương pháp $n$-step Sarsa là phương pháp tổng quát cho việc đánh giá chính sách bằng cách xác định hàm giá trị hành động. Mục tiêu cập nhật của $n$-step Sarsa cho một cặp trạng thái và hành động $(\mathit{S}_t, \mathit{A}_t)$ được xác định:
\begin{equation}
	G_{t}^{(n)} = \mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \dots + \gamma^{n-1} \mathit{R}_{t+n} + \gamma^{n} Q(\mathit{S}_{t+n}, \mathit{A}_{t+1})
\end{equation}
Sau khi xác định được mục tiêu cập nhật, giá trị của một cặp trạng thái và hành đông được cập nhật tương tự như cách cập nhật trong \ref{eq:Sarsa_action_update} của Sarsa(0):
\begin{equation}
	Q(\mathit{S}_t, \mathit{A}_t) \leftarrow Q(\mathit{S}_t, \mathit{A}_t) + \alpha \left[G_{t}^{(n)} -  Q(\mathit{S}_t, \mathit{A}_t) \right]
	\label{eq:n_Sarsa_update}
\end{equation}

\subsubsection{Phương pháp Q-learning}
\paragraph*{On-policy và off-policy}
Ý tưởng của on-policy là dựa trên những kinh nghiệm thực tế của chính hệ thống, nó có thể tự cải thiện trở nên tốt hơn. Những phương pháp thực hiện theo \textit{on-policy} là những phương pháp đánh giá và cải thiện chính sách $\pi$ dựa trên những mẫu dữ liệu có được qua việc tương tác với môi trường theo chính chính sách đó. Các phương pháp quy hoạch động, MC, TD, Sarsa là những phương pháp thực hiện theo on-policy.
Ngược lại với on-policy, ý tưởng của off-policy là hệ thống có thể cải thiện chính sách của nó dựa trên những kinh nghiệm thực tế của một hệ thống có một chính sách thực hiện khác. Những phương pháp thực hiện theo \textit{off-policy} đánh giá và cải thiện chính sách $\pi$ dựa trên những mẫu dữ liệu có được qua việc tương tác với môi trường theo một chính sách $\pi^{'}$ khác. Ngoài ra, những phương pháp thực hiện theo off-policy có thể tận dụng lại những mẫu dữ liệu cũ để tiếp tục cải thiện chính sách.

\paragraph*{Phương pháp off-policy Q-learning}
 Một trong những đột phá quan trọng nhất trong học tăng cường được phát triển dựa trên phương pháp TD được biết đến chính là Q-learning \cite{sutton1998introduction}. Tương tự như những phương pháp ở trên, phương pháp Q-learning xác định giá trị của một cặp trạng thái và hành động bằng cách cập nhật giá trị ước lượng của nó qua nhiều lần lặp:
 \begin{equation}
	 Q(\mathit{S}_t, \mathit{A}_t) \leftarrow Q(\mathit{S}_t, \mathit{A}_t) + \alpha \left[\mathit{R}_{t+1} + \gamma\max_{a}Q(\mathit{S}_{t+1}, a) - Q(\mathit{S}_t, \mathit{A}_t)\right]
 \end{equation}
Mục tiêu cập nhật giá trị của cặp trạng thái, hành động $(\mathit{S}_{t}, \mathit{A}_{t})$ bằng phương pháp Q-learning dựa trên giá trị điểm thưởng nhận được $\mathit{R}_{t+1}$ và giá trị của hành động lớn nhất ở trạng thái kế tiếp $\mathit{S}_{t+1}$. Hình \ref{fig:Q_learning_Alg} minh họa cho cách thức xác định mục tiêu cập nhật của Q-learning. 
Một điểm khác biệt của phương pháp Q-learning so với các phương pháp trên là nó xấp xỉ trực tiếp hàm giá trị hành động tối ưu $q_*$, mà không phải phụ thuộc quá nhiều vào chính sách mà nó đang theo. Ảnh hưởng của chính sách mà hệ thông đang theo đối với phương pháp này là: nó xác định cặp trạng thái và hành động nào được xuất hiện trong các mẫu thực nghiệm. Tuy nhiên để đảm bảo xác định được giá trị hành động tối ưu, một yêu cầu tối thiểu là các cặp trạng thái và hành động đều được xuất hiện trong quá trình đánh giá chính sách và giá trị các cặp trạng thái, hành động đều được cập nhật liên tục. Từng bước thực hiện của phương pháp Q-learning được trình bày trong thuật toán \ref{alg_Q_learning}.
\begin{algorithm}
	\newalgname{Thuật toán}
	\caption{Xác định hàm giá trị hành động tối ưu bằng Q-learning}
	\label{alg_Q_learning}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Đầu vào:}}
		\renewcommand{\algorithmicensure}{\textbf{Đầu ra:}}
		\algnewcommand\algorithmicoperation{\textbf{Thao tác:}}
		\algnewcommand\Operation{\item[\algorithmicoperation]}
		
		\Require
		\Ensure Hàm giá trị $Q$ xấp xỉ hàm giá trị $q_{*}$
		
		\Operation
		\Repeat
		\State Tạo một mẫu thực nghiệm $E$ bằng chính sách $\pi'$ (thực hiện $\epsilon$-greedy theo hàm giá trị $Q$ hiện tại)
		\State Khởi tạo trạng thái $S$
		\For{mỗi bước trong $E$}
		\State Chọn một hành động $A$ tại trạng thái $S$ theo chính sách $\pi'$
		\State Thực hiện hành động $A$; quan sát điểm thưởng $R$ nhận được, và trạng thái tiếp theo $S'$
		\State $Q(S,A) \leftarrow Q(S,A) + \alpha \left[R + \gamma\max_{a}Q(S', a) - Q(S,A)\right]$ \%\% Thực hiện cập nhật giá trị cho cặp trạng thái, hành động $S,A$
		\State $S \leftarrow S'$
		\EndFor
		\Until thỏa điều kiện dừng
	\end{algorithmic}
\end{algorithm}

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=4.5cm,>=stealth',bend angle=45,auto]
	
	\tikzstyle{state}=[circle,thick,draw=blue!75,fill=white!20,minimum size = 5mm, inner sep=0pt]
	\tikzstyle{action}=[circle,draw=black!75,
	fill=black!75,minimum size=2mm]
	
	\tikzstyle{every label}=[red]
	
	\begin{scope}
	\node [state, label={[name=label node]right:$\mathit{S}_{t}$}] (s0) {};
	
	\node [action, label={[name=label node]right:$\mathit{A}_{t}$}] (a0)  [below = 1 cm of s0] {}
	edge (s0);
	
	\node [state, label={[name=label node]right:$\mathit{S}_{t+1}$}] (s1) [below = 1 cm of a0]     {}
	edge node[right]{$\mathit{R}_{t+1}$} (a0);
	
	\node [action] (a1) [below right = 2 cm and 1 cm of s1, label=right:$a$]    {}
	edge(s1);
	
	\node [action] (a2) [below left  = 2 cm and 1 cm of s1]    {}
	edge (s1)
	pic[draw=black, -, angle eccentricity=1.2, angle radius=1cm]
	{angle=a2--s1--a1};
	
	\node [action] (a3) at ($(a1)!0.5!(a2)$) {}
	edge (s1);	
	\end{scope}
	\end{tikzpicture}
	\caption[Minh họa phương pháp Q-learning]{Đồ thị minh họa cập nhật hàm giá trị hành động bằng phương pháp Q-learning. Q-learning xác định mục tiêu cập nhật cho giá trị của cặp trạng thái, hành động $(\mathit{S}_{t}, \mathit{A}_t)$ bằng tổng giữa điểm thưởng $\mathit{R}_{t+1}$ nhận được khi thực hiện hành động $\mathit{A}_t$ tại trạng thái $\mathit{S}_{t}$ và giá trị hành động $a$ lớn nhất tại trạng thái kế tiếp $\mathit{S}_{t+1}$ đã nhân với hệ số $\gamma$.}
	\label{fig:Q_learning_Alg}
\end{figure}

\subsection{Phương pháp cải thiện chính sách}
\subsubsection{Phương pháp tham lam (greedy)}
Mục tiêu chúng ta xác định hàm giá trị cho một chính sách là để tìm một chính sách tốt hơn chính sách hiện tại. Giả sử chúng ta có một chính sách $\pi$ cố định tức là với mỗi trạng thái $s$, chính sách này luôn chọn thực hiện một hành động cố định, $\pi(s) = a$. Và cũng đã xác định được một hàm giá trị $v_{\pi}$ cho một chính sách đó. Với một trạng thái $s$, câu hỏi đặt ra là chúng ta nên thay đổi một chính sách cố định khác chọn hành động $a' \neq \pi(s)$ không? Chúng ta biết giá trị của trạng thái $s$ theo chính sách hiện tại $\pi$, $v_{\pi}(s)$, tốt như thế nào nhưng liệu chính sách mới $\pi$ có tốt hơn hay trở nên tệ đi? Để trả lời những câu hỏi này, chúng ta xem xét việc chọn một hành động $a$ tại trạng thái $s$ theo chính sách $\pi$. Ta có thể cải thiện một chính sách bằng cách chọn những hành động có giá trị cao nhật tại mỗi trạng thái $s$.
\begin{equation}
\label{eq:update_policy}
\pi'(s) = \underset{a \in \mathcal{A}}{\operatorname{argmax}}q_{\pi}(s,a)
\end{equation}
Với $\pi$ là chính sách cố định nên ta có hàm giá trị trạng thái cũng là hàm giá trị hành động do chính sách này chỉ sách định duy nhất một hành động tại một trạng thái $q_{\pi}(s, \pi(s)) = v_{\pi}(s)$. Cách cải thiện này cũng cho ta:
\begin{equation}
q_{\pi}(s, \pi'(s)) = \max_{a}q_{\pi}(s,a) \geq q_{\pi}(s,\pi(s)) = v_{\pi}(s)
\end{equation}
Ngoài ra với \ref{eq:improve_value_function} cho thấy rằng việc cải thiện chính sách bằng phương pháp tham lam này, ta đồng thời cũng cải thiện được hàm giá trị.
\begin{align}
v_{\pi}(s) \leqslant {} & q_{\pi}(s, \pi'(s)) = \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma v_{\pi}(\mathit{S}_{t+1}) \mid \mathit{S}_t = s\right] \notag \\
\leqslant {} & \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma q_{\pi}(\mathit{S}_{t+1}, \pi'(\mathit{S_{t+1}})) \mid \mathit{S}_t = s\right] \notag \\
\leqslant {} & \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \gamma^2 q_{\pi}(\mathit{S}_{t+2}, \pi'(\mathit{S}_{t+2})) \mid \mathit{S}_t = s\right] \notag \\
\leqslant {} & \mathbb{E}_{\pi'}\left[\mathit{R}_{t+1} + \gamma \mathit{R}_{t+2} + \dots \mid \mathit{S}_t = s\right] = v_{\pi'}(s) \label{eq:improve_value_function}
\end{align}

\subsubsection{Phương pháp $\epsilon$-greedy}